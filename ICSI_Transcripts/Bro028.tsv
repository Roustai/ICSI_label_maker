"me018"::	Eh, we should be  going.::	0
"me013"::	So ne- next week we'll have, uh, both Birger  and, uh,::	0
"me013"::	Mike - Michael -::	0
"me013"::	Michael Kleinschmidt and Birger Kollmeier will join us.::	0
"mn007"::	Uh-huh.::	0
"me013"::	Um,::	0
"me013"::	and you're -::	0
"me013"::	you're probably gonna go up in a couple - three weeks or so? When d- when are you thinking of going up to, uh,::	0
"me013"::	O_G_I?::	0
"mn007"::	Yeah, like,::	0
"mn007"::	uh, not next week but maybe the week after.::	0
"me013"::	O_K.::	0
"me013"::	Good. So at least we'll have one meeting with  yo- with you still around, and -::	0
"mn007"::	Uh-huh.::	0
"me013"::	and -::	0
"me013"::	That's good.::	0
"mn007"::	Um,::	0
"mn007"::	Yeah. Well,::	0
"mn007"::	maybe we can start with  this.::	0
"mn007"::	Mmm.::	0
"me013"::	All  today,  huh?::	0
"mn007"::	Yeah.::	0
"me013"::	Oh.::	0
"mn007"::	Um.::	0
"mn007"::	Yeah. So there was this conference call this morning,::	0
"mn007"::	um,::	0
"mn007"::	and the only topic on the agenda was just to discuss::	0
"mn007"::	a- and to::	0
"mn007"::	come at -::	0
"mn007"::	uh, to get a decision about this latency problem.::	0
"me013"::	No, this - I'm sorry, this is a conference call between different  Aurora  people or just - ?::	0
"mn007"::	Uh, yeah. It's the conference call between the Aurora,  uh, group.::	0
"me013"::	It's the main conference call.::	0
"me013"::	O_K.::	0
"mn007"::	Uh, yeah. There were like two hours of  discussions,::	0
"mn007"::	and then suddenly,::	0
"mn007"::	uh, people were tired, I guess, and they decided on::	0
"mn007"::	a number,  two hundred and twenty,::	0
"mn007"::	um,::	0
"mn007"::	included e- including everything.::	0
"mn007"::	Uh, it means that it's like eighty milliseconds  less than before.::	0
"mn007"::	Um.::	0
"me013"::	And what are we sitting at currently?::	0
"mn007"::	So, currently d- uh, we have system that has two hundred and thirty. So,::	0
"me013"::	Yeah .::	0
"mn007"::	that's fine.::	0
"me013"::	Two thirty.::	0
"mn007"::	Yeah.::	0
"mn007"::	So that's the system that's described on the second point of  this  document.::	0
"me013"::	So it's -::	0
"me013"::	we have to reduce it by ten milliseconds somehow.::	0
"mn007"::	Yeah. But that's - Yeah. That's not a problem, I - I guess.::	1
"me013"::	O_K.::	0
"mn007"::	Um.::	0
"me013"::	W-::	0
"me013"::	It's - it's p- d- primary - primarily determined by the V_A_D at this point, right?::	0
"mn007"::	Yeah.::	0
"mn007"::	Yeah. At this point, yeah.::	0
"me013"::	S- so we can make the V_A_D a little shorter. That's -::	0
"mn007"::	Yeah, uh-huh.::	0
"me013"::	Yeah. We probably should do that pretty soon so that we don't get used to it being a certain way.::	1
"mn007"::	Uh-huh.::	0
"me013"::	Yeah.::	0
"mn007"::	Um.::	0
"me013"::	Was Hari on the -::	0
"me013"::	on the phone?::	0
"mn007"::	Yeah, sure.::	0
"me013"::	O_K.::	0
"mn007"::	Well, it was mainly a discussion  between Hari and::	0
"me013"::	Hmm.::	0
"mn007"::	David, who was like -::	0
"me013"::	Yeah.::	0
"mn007"::	Uh,::	0
"me013"::	O_K.::	0
"mn007"::	mmm -::	0
"mn007"::	Uh, yeah. So,::	1
"mn007"::	the second thing is the system that we have currently.::	1
"mn007"::	Oh, yes. We have, like, a system that gives sixty-two percent::	1
"mn007"::	improvement, but::	1
"mn007"::	if you want to stick to the -::	0
"mn007"::	this latency -::	1
"mn007"::	Well, it has a latency of two thirty, but::	0
"mn007"::	if you want also to stick to the number::	0
"mn007"::	of features that - limit it to sixty,::	0
"mn007"::	then we go a little bit down but it's still sixty-one percent.::	0
"mn007"::	Uh, and if we drop the tandem network, then we have fifty-seven percent.::	0
"me013"::	Uh, but th- the two th- two thirty includes the tandem network?::	0
"mn007"::	Yeah.::	0
"me013"::	O_K.::	0
"me013"::	And i- is the tandem network, uh, small enough that it will fit on the terminal size in terms of - ?::	0
"mn007"::	Uh, no, I don't think so.::	0
"me013"::	No.::	0
"mn007"::	No.::	0
"me013"::	O_K.::	0
"mn007"::	It's still - in terms of computation, if we use, like, their way of computing the -::	0
"mn007"::	the maps - the - the MIPs,::	0
"me013"::	Mm-hmm.::	0
"me013"::	Mm-hmm.::	0
"mn007"::	I think it fits, but it's, uh, m- mainly a problem of memory.::	1
"me013"::	Right.::	0
"mn007"::	Um,::	0
"mn007"::	and I don't know how much  this can be discussed or not,::	1
"mn007"::	because it's - it could be in ROM, so it's maybe not that expensive. But -::	1
"me013"::	Ho- how much memory d- ? H- how many - ?::	0
"mn007"::	I d- I d- uh, I - I don't kn- remember exactly, but -::	1
"mn007"::	Uh. Yeah, I c- I - I have to check that.::	0
"me013"::	Yeah. I'd like to  see that, cuz maybe I could::	0
"me013"::	think a little bit about it, cuz we-::	0
"me013"::	maybe we could make it a little smaller or - I mean, it'd be - it'd be neat if we could fit it all.::	0
"mn007"::	Uh-huh.::	0
"me013"::	Uh, I'd like to see how far off::	0
"mn007"::	Mm-hmm.::	0
"me013"::	we are.::	0
"me013"::	But I guess it's still within their rules to have -::	0
"me013"::	have it::	0
"me013"::	on the, uh,::	0
"me013"::	t- uh, server side. Right?::	0
"mn007"::	Yeah.::	0
"mn007"::	Yeah.::	0
"me013"::	O_K.::	0
"mn007"::	Mmm.::	0
"me013"::	And this is still - ?::	0
"me013"::	Uh, well, y- you're saying here.::	0
"me013"::	I c- I should just let you go on.::	0
"mn007"::	Yeah, there were small tricks to make this tandem network work.::	1
"mn007"::	Uh,::	0
"mn007"::	mmm,::	0
"mn007"::	and one of the trick was to,::	1
"mn007"::	um, use::	0
"mn007"::	some kind of hierarchical structure where  the silence probability is not computed by  the final tandem network but by the V_A_D network.::	1
"mn007"::	Um,::	0
"mn007"::	so apparently it looks better when,::	1
"mn007"::	uh, we use the silence probability from the V_A_D network::	1
"mn007"::	and we re-scale the other probabilities by one minus::	0
"me013"::	Huh.::	0
"mn007"::	the silence probability.::	0
"mn007"::	Um.::	0
"mn007"::	So it's some kind of hierarchical thing,::	0
"mn007"::	uh, that Sunil also tried, um,::	0
"mn007"::	on SPINE and apparently it helps a little bit also.::	0
"mn007"::	Mmm.::	0
"mn007"::	And.::	0
"mn007"::	Yeah, the reason w- why - why we did that with the silence probability was that,::	0
"mn007"::	um -::	0
"me013"::	Could - ? Uh, uh, I'm - I'm really sorry. Can you repeat what you were saying about the silence probability? I only -::	0
"mn007"::	Mm-hmm.::	0
"mn007"::	Yeah.::	0
"me013"::	My mind was some -::	0
"mn007"::	So there is the tandem network that e- e- e- estimates the phone probabilities::	0
"me013"::	Yeah.::	0
"me013"::	Yeah.::	0
"mn007"::	and the silence probabilities also.::	0
"me013"::	Right.::	0
"mn007"::	And::	0
"mn007"::	things get better when, instead of using the silence probability computed by the::	0
"mn007"::	tandem network, we use the silence probability,::	0
"mn007"::	uh, given by the V_A_D network,::	0
"me013"::	Oh.::	0
"mn007"::	um,::	0
"me013"::	The V_A_D network is - ?::	0
"mn007"::	Which is  smaller,::	0
"mn007"::	but::	0
"mn007"::	maybe,::	0
"mn007"::	um -::	0
"mn007"::	So we have a network for the V_A_D which has one hundred hidden units,::	0
"mn007"::	and the tandem network has five hundred.::	0
"mn007"::	Um.::	0
"mn007"::	So it's smaller but th- the silence probability  from this network seems, uh, better.::	0
"me013"::	O_K.::	0
"mn007"::	Mmm.::	0
"mn007"::	Uh.::	0
"mn007"::	Well, it looks strange, but -::	0
"me013"::	Yeah. But -   O_K.::	0
"mn007"::	but it  -::	0
"mn007"::	Maybe it's - has something to do to  the fact that  we don't have infinite training data and -::	0
"me013"::	We  don't?::	0
"mn007"::	Well!::	0
"mn007"::	And::	0
"mn007"::	so - Well, things are not optimal and -::	0
"me013"::	Yeah.::	0
"mn007"::	Mmm -::	0
"me006"::	Are you - you were going to say why - what made you -::	0
"me006"::	wh- what led you to do that.::	0
"mn007"::	Yeah. Uh, there was a p-::	1
"mn007"::	problem that we observed,::	0
"mn007"::	um,::	0
"mn007"::	that there was - there were, like,::	0
"mn007"::	many insertions in the - in the system.::	1
"me013"::	Mm-hmm.::	0
"mn007"::	Mmm.::	0
"me013"::	Hmm.::	0
"mn007"::	Actually plugging in the tandem network was increasing,::	1
"mn007"::	I - I - I think, the number of insertions.::	1
"me013"::	Mm-hmm.::	0
"mn007"::	And,::	0
"mn007"::	um -::	0
"mn007"::	So it looked strange and then just using the - the other silence probability helps.  Mmm.::	1
"mn007"::	Um -::	0
"mn007"::	Yeah. The next thing we will do is train this tandem on more data.::	0
"me013"::	So, you know, in a  way  what it  might  - i- it's - it's a  little  bit like::	0
"mn007"::	Um -::	0
"me013"::	combining  knowledge  sources. Right? Because::	0
"me013"::	the fact that you have these two nets::	0
"mn007"::	Mm-hmm.::	0
"me013"::	that are different sizes  means they behave a little differently, they find different  things.::	0
"me013"::	And, um,::	0
"me013"::	if you have,::	0
"me013"::	um -::	0
"me013"::	f- the distribution that you have from,::	0
"mn007"::	Mm-hmm.::	0
"me013"::	uh, f- speech sounds::	0
"me013"::	is w-  sort of  one  source of knowledge. And this is -::	0
"me013"::	and rather than just taking one minus that to get the  other,::	0
"me013"::	which is essentially what's happening,  you have this  other::	0
"me013"::	source of knowledge that you're putting  in  there. So you make use of  both  of them::	0
"me013"::	in - in  what you're ending up with.::	0
"me013"::	Maybe it's better.::	0
"mn007"::	Yeah.::	0
"me013"::	Anyway,::	0
"me013"::	you can probably justify anything if  what's use- Yeah.::	0
"mn007"::	Yeah.::	0
"mn007"::	And - and the features are different also. I mean,::	0
"mn007"::	the V_A_D doesn't use the same features there  are .::	0
"me013"::	Mm-hmm.::	0
"me006"::	Hmm.::	0
"me013"::	Oh!::	0
"mn007"::	Um -::	0
"me013"::	That  might be the key,::	0
"mn007"::	Mm-hmm.::	0
"me013"::	actually.::	0
"me013"::	Cuz you were really thinking about speech versus nonspeech::	0
"mn007"::	Mm-hmm.::	0
"me013"::	for that.::	0
"me013"::	That's  a good point.::	0
"mn007"::	Mmm.::	0
"mn007"::	Uh. Well, there are other things that  we should do but,::	0
"mn007"::	um,::	0
"mn007"::	it requires time and -::	0
"mn007"::	We have ideas, like -::	0
"mn007"::	so, these things are like hav- having a better V_A_D.::	0
"mn007"::	Uh, we have some ideas about that. It would -::	0
"mn007"::	probably implies working a little bit on::	0
"me013"::	Mm-hmm.::	0
"mn007"::	features that are more::	0
"mn007"::	suited to::	0
"mn007"::	a voice activity detection.::	0
"mn007"::	Working on the second stream. Of course we have ideas on this also, but -::	0
"mn007"::	w- we need to try different things and -::	1
"mn007"::	Uh, but their noise estimation, um -::	0
"mn007"::	uh -::	0
"me013"::	I mean, back on the second stream, I mean, that's something we've talked about for a while. I mean,::	0
"me013"::	I think::	0
"me013"::	that's certainly a high hope.::	0
"mn007"::	Yeah.  Mmm.::	0
"me013"::	Um,::	0
"me013"::	so we have this - this default idea about just using some sort of purely spectral::	0
"me013"::	thing?::	0
"mn007"::	Uh, yeah. But,::	0
"me013"::	for a second stream?::	0
"mn007"::	um,::	0
"mn007"::	we - we did a first try with  this,  and  it  - it::	0
"mn007"::	clearly hurts.::	0
"me013"::	But, uh, how was the stream combined?::	0
"mn007"::	Uh.::	0
"mn007"::	It was c- it was just combined, um,::	0
"mn007"::	by the acoustic model. So there was,::	0
"mn007"::	no neural network for the moment. Mm-hmm.::	0
"me013"::	Right. So, I mean, if you just had a second stream that was just spectral and had another neural net and combined there,::	0
"mn007"::	Yeah. Mm-hmm.::	0
"me013"::	that -::	0
"me013"::	that, uh,::	0
"mn007"::	Mm-hmm.::	0
"me013"::	might be good.::	0
"mn007"::	Mmm.::	1
"mn007"::	Yeah.::	0
"mn007"::	Um -::	0
"mn007"::	Yeah, and the other thing,  that  noise estimation and th- um,::	0
"mn007"::	maybe try to train -::	0
"mn007"::	uh, the training data for the t-::	0
"mn007"::	tandem network, right now, is like - i- is using the noises from the Aurora task and::	0
"mn007"::	I think that people might,::	0
"mn007"::	um,::	0
"mn007"::	try to  argue   about that because::	0
"mn007"::	then in some cases we have the same noises in - for training the network  than the noises that are used for testing, and -::	0
"me013"::	Right.::	0
"mn007"::	So we have t- n-::	1
"mn007"::	uh, to try to get rid of these -::	0
"me013"::	Yeah. Maybe you just put in some  other  noise, something that's different.::	0
"mn007"::	this problem.::	1
"mn007"::	Mm-hmm.  Yeah.::	0
"me013"::	I mean, it - it's probably helpful to have - have a little noise there.::	0
"mn007"::	Uh-huh.::	0
"me013"::	But it may be something else th- at least you could say it was.::	0
"mn007"::	Yeah.::	0
"me013"::	And then - if it doesn't hurt too much, though.::	0
"mn007"::	Uh-huh.::	0
"me013"::	Yeah.  That's a good idea.::	0
"mn007"::	Um.::	0
"mn007"::	Yeah. The last thing is that::	1
"mn007"::	I  think  we are getting close to human performance.::	1
"mn007"::	Well, that's something I would like to investigate further, but,::	1
"mn007"::	um,::	0
"mn007"::	I did, like, um -::	1
"mn007"::	I did, uh, listen to the m- most noisy utterances of the SpeechDat-Car Italian and::	1
"mn007"::	tried to transcribe them.::	1
"mn007"::	And, um -::	0
"me013"::	So this is  a  particular human. This is - this i- this is  Stephane.  Yeah.::	1
"mn007"::	Yeah. So that's - that's -::	0
"me006"::	St- Stephane.::	0
"mn007"::	that's the - the flaw of the experiment. This is just - i- j-::	1
"me013"::	Yeah.::	0
"me006"::	Getting close.::	0
"mn007"::	it's just one subject, but -::	0
"mn007"::	but  still,  uh,::	1
"mn007"::	what happens is - is that,::	0
"mn007"::	uh, the digit error rate::	0
"mn007"::	on this is around one percent,::	1
"me013"::	Yeah.::	0
"mn007"::	while our system is currently at seven percent.::	1
"mn007"::	Um, but what happens also is that if::	1
"mn007"::	I listen to the, um -::	0
"mn007"::	a re-synthesized version of::	0
"mn007"::	the speech::	1
"mn007"::	and  I re-synthesized this using a white noise that's filtered by a L_P_C,::	1
"me013"::	Yeah.::	0
"mn007"::	uh, filter -::	1
"mn007"::	Um,::	0
"mn007"::	well, you can  argue,   that, uh - that this is not speech, so the ear is not::	1
"me013"::	Yeah.::	0
"mn007"::	trained to recognize this. But s- actually it sound like  whispering, so we are -::	1
"me013"::	Well, I mean, it's -::	0
"mn007"::	eh -::	0
"me013"::	There's  two  problems there. I mean - I mean, so - so the first is::	1
"mn007"::	Uh-huh.::	0
"me013"::	that by doing L_P_C-twelve with synthesized speech w- like you're saying, uh, it's -::	0
"me013"::	i- i- you're - you're adding other degradation.::	1
"me013"::	Right? So it's not just the noise but you're adding in fact some degradation because it's only an approximation.::	1
"me013"::	Um,::	0
"me013"::	and the  second  thing is - which is::	1
"me013"::	m- maybe more  interesting  -::	0
"me013"::	is that, um,::	0
"me013"::	if you do it with  whispered  speech,::	0
"me013"::	you get this number.::	1
"me013"::	What if you had  done analysis  re-synthesis and taken the  pitch  as  well?::	1
"me013"::	Alright? So now you put the pitch  in.::	1
"mn007"::	Uh-huh.::	0
"me013"::	What would the percentage be  then?::	1
"mn007"::	Um -::	1
"me013"::	See,  that's  the question.::	1
"me013"::	So, you see, if it's - if it's -::	0
"me013"::	if it's, uh - Let's say it's  back down to one percent again.::	0
"mn007"::	Uh-huh.::	0
"me013"::	That would say at least for  people,::	1
"me013"::	having the  pitch  is really, really  important,::	1
"me013"::	which would be interesting in  itself.::	0
"mn007"::	Uh, yeah. But -::	0
"me013"::	Um,::	0
"me013"::	if i- on the other hand, if it  stayed  up  near five percent,::	0
"me013"::	then I'd say "boy, L_P_C n-  twelve  is pretty  crummy ".::	0
"me013"::	You know?::	0
"mn007"::	Uh-huh.::	0
"me013"::	So I- I- I'm not sure -::	0
"me013"::	I'm not sure how we can conclude from this anything about - that our system is close to::	0
"mn007"::	Ye-::	1
"mn007"::	Yeah. Well, the point is that  eh- l- ey-  - the point is that, um,::	0
"me013"::	the human performance.::	0
"mn007"::	what I - what I listened to when I re-synthesized the L_P- the L_P_C-twelve  spectrum::	0
"mn007"::	is in a way what the system, uh, is  hearing,::	0
"mn007"::	cuz  @@  - all the - all the, um, excitation - all the -::	0
"mn007"::	well, the excitation is - is::	0
"mn007"::	not taken into account.::	0
"mn007"::	That's what we do with our system.::	0
"mn007"::	And::	0
"me013"::	Well, you're not doing the L_P_C - I mean, so - so what if you did a -::	0
"mn007"::	in this case -::	0
"mn007"::	Well, it's not L_P_C, sure, but -::	1
"me013"::	What if you did L_P_C-twenty?::	0
"mn007"::	L_P_C - ?::	0
"me013"::	Twenty.::	0
"me013"::	Right?::	0
"me013"::	I mean, th- the thing is L_P_C is not a - a really great::	1
"mn007"::	Mm-hmm.::	0
"me013"::	representation of speech.::	1
"mn007"::	Mm-hmm.::	0
"me013"::	So, all I'm saying is that you have in addition to the w- the,::	0
"me013"::	uh, removal of pitch,::	0
"mn007"::	Mm-hmm.::	1
"me013"::	you also are doing,::	0
"me013"::	uh, a particular parameterization,::	0
"me013"::	which, um,::	0
"me013"::	uh -::	0
"mn007"::	Mmm.::	0
"me013"::	Uh, so, let's see, how would you do - ? So, fo-  @@::	0
"mn007"::	But that's - that's what we do with our systems. And -::	0
"me013"::	No. Actually, we d- we - we  don't,  because we do - we do, uh,::	0
"me013"::	uh, mel filter bank, for instance. Right?::	0
"mn007"::	Yeah, but is it that -::	0
"mn007"::	is it that  different,  I mean?::	0
"me013"::	Um,::	0
"me013"::	I don't know what mel,  uh, based  synthesis  would sound like, but certainly the  spectra  are  quite  different.::	0
"mn007"::	I-::	0
"mn007"::	Mm-hmm.::	0
"mn007"::	Mm-hmm.::	0
"me018"::	Couldn't you t-::	0
"me018"::	couldn't you, um,::	0
"me018"::	test the human performance on just the original  audio?::	0
"mn007"::	This is the one percent number.::	0
"me013"::	Yeah, it's one percent. He's trying to remove the pitch information::	0
"mn007"::	Mm-hmm.::	1
"me018"::	Oh, oh. O_K, I see.::	0
"mn007"::	Mm-hmm.::	0
"me013"::	and make it closer to what - to what we're seeing as the feature vectors.::	0
"me018"::	O_K. So, y-::	0
"me013"::	@@::	0
"me018"::	uh, your performance was one percent,::	0
"mn007"::	Uh-huh.::	0
"me018"::	and then when you re-synthesize with L_P_C-twelve it went to  five.::	0
"mn007"::	Yeah.::	0
"me018"::	O_K.::	0
"me013"::	I mean -::	0
"me013"::	We were - we were j- It - it - it's a  little  bit still apples and oranges because we::	0
"me013"::	are choosing these features in order to be the best for recognition.::	0
"mn007"::	Uh-huh.::	0
"me013"::	And, um,::	0
"me013"::	i-::	0
"me013"::	if you listen to them they  still  might not be very - Even if you made something  closer::	0
"me013"::	to what we're gonna - i- it might not sound very good.::	0
"mn007"::	Yeah.::	0
"me013"::	Uh, and i- the degradation from that might -::	0
"me013"::	might actually make it even  harder,::	0
"me013"::	uh, to understand than the L_P_C-twelve. So all I'm saying is that the L_P_C-twelve::	0
"me013"::	puts in -  synthesis  puts in some degradation::	0
"mn007"::	Uh-huh.::	0
"me013"::	that's  not  what we're used to  hearing,::	0
"me013"::	and is,::	0
"me013"::	um -::	0
"me013"::	It's not -::	0
"me013"::	it's not just a question of how much information is there, as if you will always take maximum::	0
"me013"::	advantage of any information that's presented to you. In fact, you::	0
"mn007"::	Mm-hmm.::	0
"me013"::	hear  some  things better than  others.::	0
"me013"::	And so it - it isn't - But,::	0
"me018"::	But -::	0
"me013"::	I agree that it says that,::	0
"me013"::	uh, the  kind  of information that we're feeding it is probably,::	0
"me013"::	um, um, a little bit, um,::	0
"me013"::	minimal.  There's definitely some things that we've thrown away.::	0
"me013"::	And that's why I was saying it might be interesting if you -::	0
"me013"::	an interesting test of this would be if you - if you actually put the  pitch  back in.::	0
"me013"::	So, you just extract it from the actual speech and put it back in,::	0
"me013"::	and see does that - is that - does  that  make the difference?::	0
"mn007"::	Uh-huh.::	0
"me013"::	If that - if that takes it down to one percent again,::	0
"me013"::	then you'd say "O_K, it's - it's in  fact  having,::	0
"me013"::	um,::	0
"me013"::	not  just  the spectral envelope but also the -::	0
"me013"::	also the - the pitch::	0
"me013"::	that, uh,::	0
"me013"::	@@::	0
"me013"::	has the information that  people  can use, anyway."::	0
"mn007"::	Mmm.::	1
"me018"::	But from  this  it's pretty safe to say that::	0
"me018"::	the system is with- either::	0
"me018"::	two to seven percent::	0
"me018"::	away from  the performance of a  human.  Right?::	0
"me018"::	So it's somewhere in that range.::	0
"me013"::	Well, or it's - it's - Yeah, so -::	0
"me018"::	Two - two to  six  percent.::	0
"me013"::	It's - it's one point four times, uh, to, uh, seven times the error,::	0
"mn007"::	To f- seven times, yeah.::	0
"me013"::	for  Stephane.::	0
"mn007"::	Um.::	0
"me013"::	So, uh -::	0
"me013"::	@@::	0
"me013"::	uh, but i- I don't know. I- do- don't wanna take you away from other things. But that's -::	1
"mn007"::	But -  but -::	0
"me013"::	that's what - that's the first thing that  I  would be curious about, is, you know, i- i-::	0
"me013"::	when you we-::	0
"mn007"::	But the signal itself is like a mix of -::	0
"mn007"::	um, of a - a periodic sound and,   @@::	0
"mn007"::	uh, unvoiced sound, and the  noise  which is mostly,::	0
"me013"::	Mm-hmm.::	0
"mn007"::	uh,  noise.  I mean not   periodic.::	0
"mn007"::	So,  what - what do you mean exactly by putting back the pitch in? Because -::	0
"me018"::	In the L_P_C synthesis?::	0
"mn007"::	@@::	0
"me013"::	Yeah.  You did L_P_C re-synthesis -  L_ P_C re-synthesis. So,::	0
"me018"::	I think -::	0
"mn007"::	I-::	0
"mn007"::	Uh-huh.::	0
"me013"::	uh - and you did it with a noise source,::	0
"mn007"::	Mm-hmm.::	0
"me013"::	rather than with - with a s- periodic source.::	0
"me013"::	Right? So if you actually did real re-synthesis like you do in an L_P_C  synthesizer,  where it's unvoiced you use  noise,  where it's voiced you use,::	0
"me013"::	uh, periodic pulses.::	0
"mn007"::	Um.::	0
"mn007"::	Yeah, but it's neither  purely voiced or purely unvoiced.::	0
"me013"::	Right?::	0
"mn007"::	Esp- especially because there is noise.::	0
"me013"::	Well, it might be hard to do it but it- but - but the thing is that if you -::	0
"mn007"::	So -::	0
"mn007"::	Oh.::	0
"me013"::	um,::	0
"me013"::	if you detect that there's periodic - s- strong periodic components, then you can use a voiced -::	0
"mn007"::	Uh-huh.::	0
"mn007"::	Yeah.::	0
"me013"::	voice thing.::	0
"me013"::	Yeah. I mean, it's probably not worth your  time.  It's - it's a side thing and - and - and there's a lot to do. But I'm - I'm just saying, at least as a  thought  experiment,::	1
"mn007"::	Uh-huh, yeah.::	0
"mn007"::	Mm-hmm.::	0
"me013"::	that's what I would wanna  test.::	0
"me013"::	Uh, I wan- would wanna drive it with a - a - a two-source system rather than a - than a one-source system.::	0
"mn007"::	Mm-hmm.::	0
"mn007"::	Mm-hmm.::	0
"me013"::	And then that would  tell  you whether in fact it's -  Cuz we've talked about, like, this harmonic tunneling or::	0
"me013"::	other things that people have done based on pitch,::	0
"me013"::	maybe that's  really  a key element.::	0
"me013"::	Maybe - maybe, uh,::	0
"me013"::	uh, without that,::	0
"me013"::	it's - it's  not  possible to do a whole lot better than we're doing. That - that could be.::	0
"mn007"::	Yeah.::	0
"mn007"::	That's what I was thinking by doing this es- experiment, like -::	0
"me013"::	Yeah.::	0
"mn007"::	Mmm.  Evi-::	0
"me013"::	But, I mean, other than that, I don't think it's - I mean, other than the pitch de- information,::	0
"me013"::	it's hard to imagine that there's a  whole  lot more::	0
"me013"::	in the signal that -::	0
"me013"::	that, uh -::	0
"me013"::	that we're throwing away that's important.::	0
"mn007"::	Yeah, but -::	0
"mn007"::	Yeah.  Mm-hmm. Yeah, right.::	0
"me013"::	Right? I mean, we're using::	0
"me013"::	a fair number of filters in the filter bank and -::	0
"mn007"::	Mm-hmm.::	0
"me013"::	uh  -::	0
"mn007"::	Uh, yeah.::	0
"me013"::	Hmm.::	0
"me013"::	Yeah.::	0
"mn007"::	Um.::	0
"mn007"::	Yeah, that's it.::	0
"me013"::	Yeah. That  look-::	0
"me013"::	Yeah.::	1
"me013"::	That's - that's - I mean, one - one percent is sort of what I would -::	0
"me013"::	I would figure.::	0
"me013"::	If somebody was::	0
"me013"::	paying really close attention,::	0
"me013"::	you might get - I would actually think that if,::	0
"me013"::	you looked at people on various times of the day and different amounts of attention, you might actually get up to three or four percent::	0
"me013"::	error on digits. Uh,::	0
"mn007"::	Mm-hmm.::	0
"me013"::	uh -::	0
"mn007"::	Um.::	0
"me013"::	So it's -::	0
"me013"::	you know, we're not -::	0
"mn007"::	@@::	0
"me013"::	we're not::	0
"me013"::	incredibly  far off. On the other hand,::	0
"me013"::	with  any  of these numbers except maybe the one percent, it's st- it's not actually::	0
"me013"::	usable::	0
"me013"::	in a commercial system with::	0
"mn007"::	Uh-huh.::	0
"me013"::	a full telephone number or something.::	0
"mn007"::	Yeah. At these noise levels. Yeah. Mm-hmm.::	0
"me013"::	Yeah.::	0
"me013"::	Right.::	0
"mn007"::	Well, yeah. These numbers, I mean.::	0
"mn007"::	Mmm.::	0
"me013"::	Good.::	0
"me013"::	Um, while we're still on Aurora stuff  maybe you can::	1
"me013"::	talk a little about the::	0
"me013"::	status with the, uh,::	0
"me013"::	Wall Street Journal  things for it.::	1
"me018"::	So I've, um, downloaded,::	1
"me018"::	uh,::	0
"me018"::	a couple of things from Mississippi State.::	1
"me018"::	Um, one is their::	0
"me018"::	software - their,::	0
"me018"::	uh, L_V_C_S_R system.::	0
"me018"::	Downloaded the latest version of that.::	0
"me018"::	Got it compiled and everything.::	0
"me018"::	Um, downloaded::	0
"me018"::	the scripts. They wrote some scripts that sort of make it easy to run::	1
"me018"::	the system on the Wall Street Journal,::	0
"me018"::	uh,::	0
"me018"::	data.::	1
"me018"::	Um,::	1
"me018"::	so I haven't run the scripts yet.::	1
"me018"::	Uh, I'm waiting - there was one problem with part of it and I wrote a note to Joe asking him about it.::	1
"me018"::	So I'm waiting to hear from  him.::	1
"me018"::	But, um, I did print  something  out just to give you an idea about::	0
"me018"::	where the system is.::	0
"me018"::	Uh,::	0
"me018"::	they -::	0
"me018"::	on their web site they, uh, did this little  table  of where their system performs relative to::	0
"me018"::	other systems that have done this - this task.::	0
"me018"::	And, um, the Mississippi State system::	0
"me018"::	using a bigram grammar,::	0
"me018"::	uh, is at about eight point two percent.::	0
"me018"::	Other comparable systems from,::	0
"me018"::	uh -::	0
"me018"::	were getting from, uh, like six point nine, six point eight percent.::	0
"me018"::	So they're -::	0
"me013"::	This is on clean::	0
"me018"::	This is on clean -::	0
"me013"::	test set?::	0
"me018"::	on clean stuff. Yeah. They - they've started a table::	0
"me018"::	where they're showing their results on various different noise conditions but they - they don't have a whole lot of it filled in and -::	0
"me013"::	@@::	0
"me018"::	and I didn't::	0
"me018"::	notice until after I'd printed it out that,::	0
"me018"::	um,::	0
"me018"::	they don't say here  what these different testing conditions are. You actually have to click on it on the web site to see  them.::	0
"me018"::	So I - I don't know what those  numbers really  mean .::	0
"me013"::	What kind of numbers are they getting on these - on the test conditions?::	0
"me018"::	Well, see, I was a little confused because on  this  table,::	0
"me018"::	I'm -::	0
"me018"::	the- they're showing word error rate. But on  this  one,::	0
"me018"::	I - I don't know if these are word error rates because they're really big. So,::	0
"me018"::	under::	0
"me018"::	condition one here it's  ten  percent.::	0
"me018"::	Then under three it goes to sixty- four  point  six  percent.::	0
"me013"::	Yeah, that's probably Aurora. I mean -::	0
"me018"::	Yeah.::	0
"me018"::	So m- I guess maybe they're  error  rates but they're, uh -::	0
"me018"::	they're really high.::	0
"me013"::	I - I -  I  don't find that surpri- I mean, we -::	0
"me018"::	So -::	0
"me013"::	W- what's - what's some of the lower error rates on - on - on -::	0
"me013"::	uh, some of the  higher  error rates on, uh,::	0
"me013"::	some of these w- uh, uh,::	0
"me013"::	highly mismatched difficult conditions? What's a - ?::	1
"mn007"::	Uh. Yeah, it's around fifteen to twenty percent.::	0
"me018"::	Correct?::	0
"mn007"::	And the baseline, eh -::	0
"me018"::	Accuracy?::	0
"mn007"::	Uh, error rate.::	0
"me013"::	Yeah.::	0
"mn007"::	Twenty percent error rate,::	0
"me013"::	Yeah. So twenty percent error rate on digits.::	0
"mn007"::	and -::	0
"me013"::	So if you're doing - so if you're doing,::	0
"mn007"::	and -::	0
"me018"::	Oh, oh, on digits. Yeah.::	0
"mn007"::	On  digits.   And this is so - so - still the baseline.  Right?::	0
"me018"::	O_K.::	0
"me013"::	you know,::	0
"me013"::	@@::	0
"me013"::	sixty-thousand - Yeah, and if you're saying sixty-thousand word recognition, getting sixty percent error on some of these noise condition-  not  at all surprising.::	0
"me018"::	Yeah.::	0
"me018"::	Yeah.::	0
"mn007"::	The baseline is sixty percent also on digits,::	0
"me018"::	Oh, is it?::	0
"mn007"::	on the m- more  mismatched conditions.::	0
"me018"::	O_K.::	0
"mn007"::	So.::	0
"me013"::	Yeah.::	0
"me018"::	So, yeah, that's probably what it is then.::	0
"me018"::	Yeah. So they have a lot of different conditions that they're gonna be filling  out .::	0
"me013"::	It's a  bad  sign when you - looking at the numbers, you can't tell whether it's accuracy or error rate.::	1
"me018"::	Yeah.::	0
"me018"::	Yeah. It's - it's gonna be hard.::	0
"me018"::	Um,::	0
"me018"::	they're - I- I'm still waiting for them to  release the, um,::	1
"me018"::	multi-C_P_U version of their scripts, cuz right now their script only handles::	1
"me018"::	processing on a single C_P_U,::	1
"me018"::	which will take a  really  long time::	1
"me018"::	to run. So.::	1
"me013"::	This is for the training?::	0
"me018"::	But their s-::	0
"me018"::	Uh -::	0
"me018"::	I beli-  Yes,  for the training  also.::	0
"me013"::	O_K.::	0
"me018"::	And, um, they're supposed to be coming out with it any time, the multi-C_P_U one.::	0
"me013"::	O_K.::	0
"me018"::	So, as soon as they get that, then I'll - I'll grab those too and so w-::	1
"me013"::	Yeah. Cuz we have to get started, cuz it's - cuz, uh,::	1
"me018"::	Yeah.::	0
"me018"::	Yeah. I'll go ahead and try to run it though with just the single C_P_U one, and - I - they - they,::	1
"me013"::	if the -::	0
"me018"::	um, released like a  smaller  data set that you can use that only takes like sixteen hours to train and stuff.::	1
"me018"::	So I can - I can run it on that just to make sure that the -::	0
"me013"::	Oh! Good.::	0
"me013"::	Yeah.::	0
"me018"::	the thing works and everything.::	1
"me006"::	Hmm.::	0
"me013"::	Cuz we'll -::	0
"me013"::	I guess the actual evaluation will be in six weeks or something.::	0
"me013"::	So.::	0
"me013"::	Is that about right  you think?::	0
"mn007"::	Uh, we don't know yet, I - I think.::	0
"me013"::	Really, we don't know?::	0
"mn007"::	Uh-huh.::	0
"mn007"::	Um.::	0
"me013"::	Hmm.::	0
"me018"::	It wasn't on the conference call this morning?::	0
"mn007"::	No.::	0
"me018"::	Hmm.::	0
"me018"::	Did they say anything::	0
"me018"::	on the conference call  about, um,::	0
"me018"::	how the  Wall Street Journal part of the test was going to be  run?::	0
"me018"::	Because I -::	0
"mn007"::	No.  Mmm.::	0
"me018"::	I thought I remembered hearing that  some  sites::	0
"me018"::	were saying that they didn't have the  compute  to be able to run the Wall Street Journal stuff::	0
"me018"::	at  their   place,::	0
"me018"::	so there was some talk about having Mississippi State  run   the systems  for  them.::	0
"me018"::	And I -::	0
"me018"::	Did - did that come up at all?::	0
"mn007"::	Uh, no. Well, this -::	0
"mn007"::	first, this was not the point at all of this -::	0
"me018"::	Oh, O_K.::	0
"mn007"::	the meeting today and,::	0
"me013"::	Some-::	0
"mn007"::	uh, frankly, I don't know because I d-::	0
"mn007"::	didn't  read   also the  most recent mails about::	0
"mn007"::	the large-vocabulary task. But,::	0
"mn007"::	uh, did you - do you still,::	0
"mn007"::	uh, get the mails?::	0
"mn007"::	You're not on the mailing list or what?::	0
"me018"::	Hmm-mm.::	0
"me018"::	The only, um, mail I get is from Mississippi State -::	0
"mn007"::	Uh-huh.::	0
"me018"::	so -::	0
"mn007"::	Oh, yeah. So we should have a look at this.::	0
"me018"::	about their system. I - I don't get any  mail about -::	0
"me013"::	I have to say, there's  uh  something funny-sounding about saying that one of these big companies doesn't have enough  cup-   compute power do that, so they're having to have it done by Mississippi State.::	0
"me018"::	Yeah.::	0
"me013"::	It just -  just  sounds  funny.::	0
"me013"::	But,::	0
"me018"::	Yeah. It does.::	0
"me013"::	anyway.::	0
"me018"::	Yeah. I'm - I'm wondering about that because::	0
"me018"::	there's this whole issue about,::	0
"me018"::	you know,::	0
"me018"::	simple tuning parameters, like word insertion penalties.::	0
"mn007"::	Mm-hmm.::	0
"me018"::	And  whether or not those are::	0
"me018"::	going to be tuned or not, and -::	0
"me018"::	So.::	0
"mn007"::	Mm-hmm.::	0
"me018"::	I mean, it makes a big difference. If you change your front-end,::	0
"me018"::	you know,::	0
"me018"::	the  scale  is completely -  can  be completely different, so.::	0
"me018"::	It seems reasonable that  that  at least should be::	0
"me018"::	tweaked to match the front-end.::	0
"me018"::	But -::	0
"mn007"::	You didn't get any answer from  Joe?::	0
"me018"::	I  did,  but Joe  said,::	0
"mn007"::	Uh-huh.::	0
"me018"::	you know, "what you're saying makes sense  and  I don't know".::	0
"mn007"::	Uh-huh.::	0
"me018"::	So  he  doesn't know what::	0
"me018"::	the answer is. I mean, that's th- We had this::	0
"me018"::	back and forth a little bit about,::	0
"me018"::	you know, are sites gonna - are you gonna run this data for different sites?::	0
"me018"::	And, well, if -::	0
"me018"::	if Mississippi State runs it, then maybe they'll::	0
"me018"::	do a little optimization::	0
"me018"::	on that  parameter,::	0
"me018"::	and, uh -::	0
"me018"::	But then he wasn't  asked  to run it for anybody. So i- it's -::	0
"me018"::	it's just not clear yet what's gonna happen.::	0
"mn007"::	Mm-hmm.::	0
"me018"::	Uh, he's been putting this stuff out on their web site and - for people to grab but::	0
"me018"::	I haven't heard too much about::	0
"me018"::	what's happening.::	0
"me013"::	So it  could  be - I mean, Chuck and I had actually talked about this a couple times, and - and - over some lunches, I think,::	1
"me013"::	that, um,::	0
"me013"::	one thing that we might wanna do -::	1
"me013"::	The- there's this question about, you know, what do you wanna scale?::	1
"me013"::	Suppose y- you  can't  adjust::	0
"me013"::	these word insertion penalties and so forth, so you have to do everything at the level of the features. What could you do?::	1
"me013"::	And, uh, one thing I had suggested at an earlier time was maybe some sort of scaling, some sort of root or - or something of::	1
"me013"::	the, um,::	0
"me013"::	uh,  features.::	1
"me013"::	But the problem with that is that isn't  quite  the same, it occurred to me later,::	1
"me013"::	because what you really want to do is scale the,::	1
"me013"::	uh,  @@   the range of the likelihoods rather than -::	1
"mn007"::	Nnn, the dist-::	0
"mn007"::	Yeah.::	0
"me013"::	But,::	0
"me013"::	what  might  get at something similar, it just occurred to me, is kind of an intermediate thing - is because we do this strange thing that we do with the tandem system,::	0
"me013"::	at least in  that  system what you could do::	0
"me013"::	is take the, um,::	0
"me013"::	uh,::	0
"me013"::	values that come out of the  net,  which are something  like  log probabilities,::	0
"me013"::	and scale  those.::	0
"me013"::	And then, uh, um -  then at least::	0
"me013"::	those  things would have::	0
"me013"::	the right values or the right - the right range.::	0
"me013"::	And then that goes into the rest of it and then that's used as observations. So it's - it's,::	0
"mn007"::	Mm-hmm.::	0
"me013"::	um,::	0
"mn007"::	Mm-hmm.::	0
"me013"::	another way to do it.::	0
"mn007"::	But, these values are not::	0
"mn007"::	directly used as probabilities anyway. So there are - there is -::	0
"me013"::	I know they're not.::	0
"me013"::	I know they're not. But - but,::	0
"me013"::	you know -::	0
"mn007"::	Uh-huh.::	0
"me013"::	So because what we're doing is pretty strange and complicated, we don't really know what the effect is::	0
"mn007"::	Mm-hmm.::	0
"me013"::	at the other end. So,::	0
"me013"::	um,  my thought was maybe - I mean, they're  not  used as probabilities,::	0
"me013"::	but::	0
"me013"::	the log probabilities -::	0
"me013"::	we're taking advantage of the fact that something like log probabilities has more of a Gaussian shape than Gaus- than::	0
"me013"::	probabilities, and so we can model them better. So,  in a way we're::	0
"me013"::	taking advantage of the fact that they're probabilities, because::	0
"me013"::	they're this  quantity  that::	0
"me013"::	looks kind of Gaussian when you take it's log. So,   uh, maybe -::	0
"mn007"::	Mm-hmm.::	0
"me013"::	maybe  it would have a - a reasonable effect to do that. I d- I don't know.::	0
"me013"::	But,  I mean, I guess we still haven't had a -::	1
"me013"::	a ruling  back  on this. And we  may  end up being in a situation where we just  you know  really  can't  change the::	1
"me013"::	word insertion penalty. But the  other  thing we could do::	1
"me013"::	is - also  we   could - I mean, this - this may not help us,::	1
"me013"::	uh, in the  evaluation  but it might help us in our  understanding  at least. We might,  just  run  it with different insper-  insertion  penalties,::	1
"me013"::	and show that, uh, "well, O_K,  not  changing it,::	1
"me013"::	playing the rules the way you wanted, we did  this.  But in fact if we did  that,  it made a -  a big difference."::	1
"me018"::	I wonder if it -::	0
"me018"::	it might be possible to,::	0
"me018"::	uh, simulate::	0
"me018"::	the back-end::	0
"me018"::	with some other::	0
"me018"::	system. So we - we get our f- front-end features,::	0
"me018"::	and then, uh,::	0
"me018"::	as part of the process of figuring out the scaling::	0
"me018"::	of these features,  you know, if we're gonna take it to a root or to a power or something,::	0
"me013"::	Mm-hmm.::	0
"me018"::	we have some back-end::	0
"me018"::	that we attach onto our features that::	0
"me018"::	sort of::	0
"me018"::	simulates what would be happening.::	0
"me018"::	Um,::	0
"me013"::	And just adjust it until it's the best number?::	0
"me018"::	and just adjust it until that -::	0
"me018"::	our  l-   version  of the back-end,::	0
"me018"::	uh,::	0
"me018"::	decides that - that -::	0
"me013"::	Well, we can probably use the  real  thing, can't we? And then jus- just, uh,::	0
"me018"::	Yeah. Oh, yeah.::	0
"me013"::	use it on a reduced test set or something.::	0
"me018"::	That's true.::	0
"me013"::	Yeah.::	0
"me018"::	And then we just use that to determine some  scaling  factor that we use.::	0
"me013"::	Yeah. So I mean, I- I think that that's a reasonable thing to do and the only question is what's the actual knob that we use? And the knob that we use should -::	0
"me018"::	Mm-hmm.::	0
"me013"::	uh, uh, unfortunately, like I say, I don't know the analytic solution to this cuz what we really want to do is change the scale of the likelihoods, not the cha- not the scale of the -::	0
"me018"::	Mm-hmm.::	0
"me013"::	the  observations. But -::	0
"me013"::	but, uh -::	0
"mn007"::	Mm-hmm.::	0
"me018"::	Yeah.::	0
"me006"::	Out of curiosity, what - what kind of recognizer  is the one from Mississippi State?::	0
"me018"::	Uh, w- what do you mean when you say "what kind"?::	0
"me006"::	Is it - ?::	0
"me006"::	Um, is it like a  Gaussian mixture model?::	0
"me018"::	Yeah.::	0
"me018"::	Gaussian mixture model.::	0
"me006"::	O_K.::	0
"me018"::	It's the same system that they use  when they participate in the Hub-five evals. It's a,::	0
"me018"::	um -::	0
"me018"::	sort of  came out of, uh -::	0
"me018"::	uh, looking a lot like H_T_K. I mean, they started off with - um, when they were building their system they were always comparing to H_T_K to make sure they were getting similar results. And so,::	0
"me018"::	it's a Gaussian mixture system,::	0
"me018"::	uh -::	0
"me013"::	Do they have the same sort of mix-down sort of procedure, where they::	0
"me013"::	start off with a small number of some things and - ?  Yeah.::	0
"me018"::	I don't know.::	0
"me018"::	Yeah. And then  divide the mixtures in half. I don't know if they do that. I'm not really sure.::	0
"me013"::	Yeah.::	0
"me006"::	Hmm.::	0
"me013"::	D- Do you know what kind of  tying  they use? Are they - they sort of - some sort of -::	0
"me013"::	a bunch of Gaussians that they share across everything? Or -::	0
"me018"::	Yeah, th- I have - I - I - I don't have it up here but I have a -  the whole system description,::	0
"me013"::	or if it's - ?::	0
"me018"::	that describes::	0
"me018"::	exactly what their  system is and I - I'm not sure.::	0
"me013"::	O_K.::	0
"me018"::	But, um -::	0
"me013"::	O_K.::	0
"me018"::	It's some kind of a mixture of Gaussians and,::	0
"me018"::	uh,  clustering  and,::	0
"me018"::	uh -::	0
"me018"::	They're - they're trying to put in sort of all of the  standard  features::	0
"me018"::	that people use nowadays.::	0
"me006"::	Mm-hmm.::	0
"me013"::	So the other, uh, Aurora thing maybe is - I- I dunno if any of this is gonna::	0
"me013"::	come in in time to be relevant, but, uh, we had talked about, uh,  Guenter::	0
"me013"::	playing around,::	0
"me013"::	uh,::	0
"mn007"::	Mm-hmm.::	0
"me013"::	uh, over in Germany and - and,  @@::	0
"me013"::	uh,  possibly coming up with something::	0
"me013"::	that would, uh,  uh, fit in later. Uh, I saw that other mail where he said that he -::	0
"me013"::	uh,  it  wasn't going to work for him to do C_V_S.::	0
"mn007"::	Yeah.::	0
"mn007"::	Yeah. So now he has a version of the software.::	0
"me013"::	So he just has it all sitting there. Yeah.::	0
"mn007"::	Yeah. Um -  Mm-hmm.::	1
"me013"::	So if he'll -::	0
"me013"::	he might work on improving the noise estimate  or on::	0
"me013"::	some histogram things, or -::	0
"mn007"::	Yeah. Mm-hmm.::	1
"me013"::	Yeah. I just saw the Eurospeech - We - we didn't talk about it at our meeting but I just saw the - just read the paper.::	0
"me013"::	Someone, I forget the name,  and - and Ney, uh, about histogram::	0
"me013"::	equalization?::	0
"me013"::	Did you see that one?::	0
"mn007"::	Um, it was a  poster.  Or -::	0
"me013"::	Yeah. I mean, I just read the paper. I didn't see the poster.::	0
"mn007"::	Yeah.::	0
"mn007"::	Yeah.::	0
"mn007"::	Um -::	0
"mn007"::	It was something  similar to n-  on-line normalization finally - I mean, in  the idea of - of normalizing -::	1
"me013"::	Yeah. But it's a little more - it - it's a little finer, right? So they had like ten quantiles and -::	0
"mn007"::	Yeah.::	0
"mn007"::	Right.::	0
"me013"::	and they adjust the distribution. So you - you have the distributions from the training set,::	0
"mn007"::	N-::	1
"me013"::	and then, uh -::	0
"me013"::	So this is just a - a histogram of - of::	0
"me013"::	the  amplitudes,  I guess. Right? And then -::	0
"mn007"::	Mm-hmm.::	0
"me013"::	Um, people do this in image processing some.  You have this kind of -::	0
"me013"::	of histogram of - of levels of brightness or whatever. And - and - and then,::	0
"me018"::	Hmm.::	0
"me013"::	when you get a  new   - new thing that you - you want to adjust to be  better in some way,::	0
"me013"::	you adjust it so that the histogram of the  new  data looks like the  old  data. You do this kind of  piece-wise  linear   or,::	0
"me013"::	uh,  some  kind of piece-wise approximation. They did a -::	0
"me013"::	uh  one version that was piece-wise linear and another that had a power law thing between  them   -::	0
"me013"::	between the  points. And, uh,::	0
"me013"::	they said they s- they sort of see it in a way as s- for the speech case  - as being kind of a generalization of spectral subtraction in a way, because, you know, in spectral subtraction you're trying to::	1
"me013"::	get rid of this excess energy. Uh, you know, it's not supposed to be there.::	1
"me013"::	Uh -  and, uh, this is sort of   adjusting it for - for a  lot  of different levels. And then they have s- they have some kind of,::	0
"me013"::	uh,  a floor or something, so if it gets too low you don't -::	0
"me006"::	Hmm.::	0
"me018"::	Hmm.::	0
"me013"::	don't do it. And they - they claimed very nice results, and -::	0
"mn007"::	Mm-hmm.::	0
"me018"::	So is  this  a histogram across different frequency bins?::	0
"me018"::	Or - ?::	0
"me013"::	Um, I  think  this i-::	0
"me013"::	You know, I don't remember that.::	0
"me013"::	Do you remember - ?::	0
"mn007"::	I  think  they have, yeah, different histograms.  I- uh -::	0
"mn007"::	Something like one per  frequency band, or -  But I did -::	0
"me013"::	One -::	0
"me018"::	So, one histogram per frequency bin.::	0
"me013"::	One per critical -::	0
"mn007"::	Yeah, I guess.::	0
"mn007"::	But I should read the paper. I just went  through the poster quickly, and I didn't -::	1
"me018"::	And that's -::	0
"me013"::	Yeah. And I don't remember whether it was  filter bank things or whether it was F_F_T bins or -::	1
"me018"::	So th-::	0
"me018"::	Oh.::	0
"me018"::	Huh.::	0
"me018"::	And - and that -::	0
"me013"::	I don't remember that.::	0
"me018"::	that, um,  histogram represents  the  different energy levels that have been seen at that  frequency?::	0
"me013"::	And how often they - you've seen them. Yeah.::	0
"me006"::	Hmm.::	0
"me018"::	Uh-huh.::	0
"me013"::	Yeah. And they do - they said that they could do it for the test -::	0
"me013"::	So you don't have to change the training. You just do a measurement over the training.::	0
"me013"::	And then, uh, for testing, uh, you can do it for one per utterance.::	0
"me013"::	Even relatively short utterances. And they claim it - it works pretty well.::	0
"me018"::	So they, uh -::	0
"me018"::	Is the idea that you - you run a test utterance::	0
"me018"::	through some histogram generation thing and then you  compare  the histograms and that tells you::	0
"me013"::	I guess in pri-::	0
"me018"::	what to do to the utterance to::	0
"me013"::	Yeah.::	0
"me018"::	make it more like - ?::	0
"me013"::	In principle. I didn't read carefully how they actually implemented it, whether it was some,::	0
"me018"::	I see.::	0
"me018"::	Hmm.::	0
"me018"::	Yeah.::	0
"me013"::	uh, on-line thing, or whether it was a second pass, or what. But -::	0
"me013"::	but they -::	0
"me018"::	Hmm.::	0
"me013"::	That - that was sort of the idea. So that - that seemed, you know, different.::	0
"me013"::	We're sort of curious about, uh, what are some things that are,::	0
"me013"::	u- u- um,::	0
"me013"::	@@::	0
"me013"::	conceptually  quite different from what we've done.::	0
"me013"::	Cuz we - you know, one thing that w- that,::	0
"me018"::	Mm-hmm.::	0
"me013"::	uh, Stephane and Sunil seemed to find,::	0
"me013"::	uh, was, you know, they could actually make a unified piece of software that handled a  range  of different things that people were talking about, and it was really just sort of setting of different  constants.::	0
"me013"::	And it would turn, you know, one thing into another. It'd turn Wiener filtering into spectral subtraction, or whatever.::	0
"me013"::	But there's other things that we're  not  doing. So, we're  not  making any use of pitch,::	0
"me013"::	uh, uh, which again, might - might be important,::	0
"me013"::	uh, because the stuff between the harmonics is probably a schmutz. And - and the,::	0
"me013"::	uh, transcribers will have fun with that.::	0
"me013"::	Uh -::	0
"me013"::	And, um,::	0
"me013"::	the, uh, stuff at the harmonics isn't so much.::	0
"me013"::	And - and, uh -::	0
"me013"::	And we- there's this overall idea of really sort of::	0
"me013"::	matching the - the hi- distributions somehow.::	0
"me013"::	Uh, not just, um,::	0
"me013"::	um -::	0
"me013"::	not just subtracting off your estimate of the noise.::	0
"me013"::	So.::	0
"me013"::	So I guess, uh,::	0
"me013"::	Guenter's gonna::	0
"me013"::	play around with some of these things now over this next  period, or - ?::	0
"mn007"::	Uh,::	0
"mn007"::	I dunno. I don't have feedback from him, but::	0
"me013"::	Yeah.::	0
"mn007"::	I guess he's gonna, maybe -::	0
"me013"::	Well, he's  got  it anyway, so he  can.::	0
"mn007"::	Yeah.::	1
"mn007"::	Uh-huh.::	0
"me013"::	So potentially if he came up with something that was useful, like a diff- a better noise estimation module or something, he could ship it to you guys  u-  up there and::	0
"mn007"::	Yeah.::	0
"me013"::	we could put it in.::	0
"mn007"::	Mm-hmm.  Mm-hmm.::	1
"me013"::	Yeah.::	0
"me013"::	Yeah. So,::	0
"me013"::	that's good.::	0
"me013"::	So, why don't we just, uh, um - I think starting -::	0
"me013"::	starting a w- couple weeks from now, especially if you're not gonna be around for a while, we'll - we'll be shifting more over to some other -::	0
"me013"::	other territory. But, uh,::	0
"me013"::	uh,::	0
"me013"::	uh, n- not - not  so  much in this meeting about Aurora, but -::	0
"me013"::	but, uh,::	0
"me013"::	uh, maybe just, uh, quickly today about - maybe::	0
"me013"::	you could just say a little bit about what you've been talking about with Michael. And -::	0
"me013"::	and then Barry can say something about  what  - what we're talking about.::	0
"me026"::	O_K.::	0
"me026"::	So Michael Kleinschmidt, who's a P_H_D student from Germany,::	1
"me026"::	showed up  this  week. He'll be here for about six months. And he's done some work using::	1
"me026"::	an  auditory  model::	0
"me026"::	of, um,::	0
"me026"::	human hearing,::	1
"me026"::	and  using that f- uh, to generate speech recognition features.::	1
"me026"::	And  he did  work back in Germany::	1
"me026"::	with, um, a  toy  recognition system::	0
"me026"::	using, um, isolated::	0
"me026"::	digit recognition  as the task. It was actually just a  single-layer   neural  network::	1
"me026"::	that classified words - classified digits,::	0
"me026"::	in fact.::	0
"me026"::	Um, and  he tried that on - I  think  on some Aurora data and got results that he thought  seemed respectable. And::	0
"me026"::	he w- he's coming here to u- u- use it on a-::	1
"me026"::	uh, a real speech recognition system.::	1
"me026"::	So I'll be working with him on that. And, um,::	0
"me026"::	maybe I should say a little more about these features,::	0
"me026"::	although I don't understand them  that  well.::	0
"me026"::	The - I think it's a two-stage idea. And, um,::	0
"me026"::	the first stage of these features::	0
"me026"::	correspond to what's called the peripheral  auditory system.::	0
"me026"::	And::	0
"me026"::	I guess  that  is like::	0
"me026"::	a filter bank with a compressive nonlinearity.::	0
"me026"::	And::	0
"me026"::	I'm- I'm not sure  what  we have::	0
"me026"::	@@   in  there that isn't already modeled in something like,::	0
"me026"::	um,  P_L_P. I should learn more about that.::	0
"me026"::	And then::	0
"me026"::	the  second  stage  is, um,::	0
"me026"::	the  most  different thing, I think, from what we usually do. It's, um -::	0
"me026"::	it computes features which are,::	0
"me026"::	um,::	0
"me026"::	based on -::	0
"me026"::	sort of like based on diffe- different w- um, wavelet basis functions::	0
"me026"::	used to analyze::	0
"me026"::	the input.::	0
"me026"::	@@::	0
"me026"::	So th- he uses analysis functions called  Gabor functions,::	0
"me026"::	um,  which have a certain::	0
"me026"::	extent, um,::	0
"me026"::	in time and in frequency.::	0
"me026"::	And::	0
"me026"::	the idea is these are used to sample,::	0
"me026"::	um,::	0
"me026"::	the signal in a-::	0
"me026"::	represented as a time-frequency representation.::	0
"me026"::	So you're  sampling some piece of this time-frequency plane.::	0
"me026"::	And, um,::	0
"me026"::	that,::	0
"me026"::	um, is - is interesting, cuz,::	0
"me026"::	@@  for - for one thing, you could use it,::	0
"me026"::	um, in a - a multi-scale way. You could have these -::	0
"me026"::	instead of having everything - like we use a twenty-five millisecond or so analysis window,::	0
"me026"::	typically,::	0
"me026"::	um, and that's our time scale for features, but you could -::	0
"me026"::	using this, um, basis function idea, you could have  some  basis functions::	0
"me026"::	which have a lot  longer  time scale::	0
"me026"::	and, um, some which have a lot  shorter,  and::	0
"me026"::	so it would be like  a set of  multi-scale  features.::	0
"me026"::	So he's interested in, um -::	0
"me026"::	Th- this is -  because  it's, um - there are these different parameters for the shape of these::	1
"me026"::	basis functions,::	0
"me026"::	um -::	0
"me026"::	there are a  lot  of different possible basis functions. And so he -::	1
"me026"::	he actually does::	0
"me026"::	an optimization procedure to choose an -::	0
"me026"::	an optimal set of basis functions out of all the possible ones.::	1
"me018"::	Hmm.  H-::	0
"me018"::	What does he do to choose those?::	0
"me026"::	The method he uses is kind of funny - is,::	1
"me026"::	um,::	0
"me026"::	he starts with - he has a set of M_ of them.::	1
"me026"::	Um,::	0
"me026"::	he - and then  he uses that to classify -::	0
"me026"::	I mean, he t- he tries, um,::	1
"me026"::	using  just M_ minus one of them.::	1
"me026"::	So there are M_ possible subsets of this::	1
"me026"::	length-M_ vector.  He tries classifying, using each of the M_::	1
"mn007"::	Hmm.::	1
"me026"::	possible  sub-vectors.::	1
"me026"::	Whichever sub-vector,::	1
"me026"::	um,::	0
"me026"::	works the -::	0
"me026"::	the  best,  I guess, he says -::	0
"me013"::	Y- yeah.::	0
"me026"::	the - the fe- feature  that  didn't use::	0
"me013"::	Gets thrown out.::	0
"me026"::	was the most useless feature,::	1
"me013"::	Yeah.::	0
"me026"::	so we'll throw it out and we're gonna randomly select::	1
"me026"::	another feature::	0
"me026"::	from the set of possible basis functions.::	1
"me018"::	Hmm!::	0
"me013"::	Yeah.  So i- so it's actuall-::	0
"me018"::	So it's a -::	0
"me018"::	it's a  little  bit like a genetic algorithm or something in a way.::	0
"me006"::	It's like a greedy -::	0
"me013"::	Well, it's - it's much simpler. But it's - but it's - uh, it's - there's a lot - number of things I like about it, let me just say.::	0
"me018"::	Greedy.::	0
"me013"::	So, first thing, well, you're absolutely right. I mean,::	0
"me013"::	i- i-::	0
"me013"::	in truth,::	0
"me013"::	both  pieces of this are - have their analogies in stuff we already do.::	0
"me013"::	But it's a different  take::	0
"me013"::	at how to approach it and  potentially  one that's::	0
"me013"::	m- maybe a bit more  systematic  than what we've done,::	0
"me013"::	uh, and a b- a  bit  more inspiration from - from auditory things. So it's - so I think it's a neat thing to try.::	0
"me013"::	The  primary  features,::	0
"me013"::	um,  are  in fact -::	0
"me013"::	Yeah, essentially, it's - it's, uh, you know, P_L_P or - or mel cepstrum, or something like that. You've - you've got some,::	0
"me013"::	uh,  compression.  We always have some compression. We always have some - you know, the - the -::	0
"me013"::	the kind of filter bank with a kind of::	0
"me013"::	quasi-log scaling.::	0
"me013"::	Um,  if you put in - if you also include the  RASTA  in it - i- RASTA - the filtering being done in the log domain::	0
"me013"::	has an A_G_C-like, uh, characteristic, which, you know, people typi- typically put in these kind of,::	0
"me013"::	uh,  um,::	0
"me013"::	uh, auditory front-ends. So it's very, very similar,::	0
"me013"::	uh, but it's not exactly the same.::	0
"me013"::	Um,::	0
"me013"::	I would agree that the second one is - is  somewhat  more different but,::	1
"me013"::	um, it's  mainly  different in that the things that we have been  doing   like that::	0
"me013"::	have been -::	0
"me013"::	um,::	0
"me013"::	had a different kind of motivation and have ended up with different kinds of constraints.::	0
"me013"::	So, for instance, if you look at the L_D_A RASTA stuff,::	0
"me013"::	you know,  basically  what they do is they - they look at the different eigenvectors out of the L_D_A and they form filters out of it. Right?::	0
"me013"::	And those   filters  have different, uh, kinds of temporal extents and temporal characteristics.::	0
"me013"::	And so in fact they're multi- scale.::	0
"me013"::	But, they're not sort of systematically multi-scale, like "let's start here and go to there, and go to there, and go to there", and so forth. It's more like,::	0
"me013"::	you run it on this, you do discriminant analysis, and you find out what's helpful.::	0
"me026"::	I- it's multi-scale because you use several of these in parallel, is that right? Of -::	0
"me013"::	@@::	0
"me013"::	Yeah. They use several of them. Yeah.::	0
"me026"::	O_K.::	0
"me013"::	Uh, I mean, you don't  have  to but - but - but, uh, Hynek has.::	0
"me013"::	Um,::	0
"me013"::	but it's  also,  uh -::	0
"me013"::	@@::	0
"me013"::	Hyn-  when Hynek's had people do this kind of L_D_A analysis, they've done it on  frequency  direction and they've done it on the  time  direction.::	0
"me013"::	I  think  he  may  have had people sometimes doing it on both  simultaneously   - some two-D_ - and that would be the closest to these Gabor function kind of things.::	0
"me013"::	Uh, but I don't think they've done that much of that.::	0
"me013"::	And, uh, the  other  thing that's interesting - the - the, uh -::	0
"me013"::	the  feature  selection thing, it's a  simple  method,::	0
"me013"::	but I kinda  like  it. Um,::	0
"me013"::	there's a -  a old, old method for feature selection. I mean,::	0
"me013"::	eh, uh, I remember people referring to it as old when I was playing with it twenty years ago, so I know it's pretty old,::	0
"me013"::	uh, called Stepwise Linear Discriminant Analysis in which you - which - I think it's used in social sciences a lot.::	0
"me013"::	So, you - you - you - you pick the best feature.::	0
"me013"::	And then::	0
"me013"::	you take - y- you find the next feature that's the best in  combination  with it.::	0
"me013"::	And then so on and so on.::	0
"me013"::	And what - what  Michael's  describing seems to me  much,  much  better,::	0
"me013"::	because the  problem  with the stepwise discriminant analysis is that you don't know that - you know, if you've  picked the right::	0
"me013"::	set of features. Just because something's a good feature doesn't mean that you should be  adding  it. So,::	1
"me013"::	um,::	0
"me013"::	uh,  here  at least you're starting off with  all  of them,  and you're  throwing out  useless  features. I think that's - that seems, uh -  that seems like a lot better idea.::	1
"me013"::	Uh, you're  always  looking at things in combination with other features.::	0
"me013"::	Um,::	0
"me013"::	so the only thing is, of course, there's this - this::	0
"me013"::	artificial question of - of, uh,::	0
"me013"::	exactly how you -::	0
"me013"::	how you a- how you assess it and if - if your order had been different in throwing them out.::	0
"me013"::	I mean, it still isn't necessarily really  optimal,  but it seems like a pretty good heuristic.::	1
"me006"::	Hmm.::	0
"me013"::	So I th- I think it's - it's - I think it's kinda neat stuff. And - and - and, uh,::	1
"me013"::	the thing that  I  wanted to - to add to it also was to have us use this in a multi-stream way.::	1
"me006"::	Hmm.::	0
"me013"::	Um,::	0
"me013"::	so - so that, um,::	1
"me013"::	when you come up with these different things,::	0
"me013"::	and these different functions,::	0
"me013"::	you don't necessarily just put  them   all into one huge vector,::	1
"me013"::	but perhaps  you::	1
"me013"::	have some of  them   in one stream and some of  them   in another stream, and so forth.::	1
"me013"::	And, um,::	0
"me013"::	um,::	0
"me013"::	um -::	0
"me013"::	And we've also talked a little bit about, uh,::	0
"me013"::	uh, Shihab Shamma's stuff, in which::	1
"me013"::	you - the way you look at it is that there's these different mappings and some of them emphasize, uh,  upward  moving,::	1
"me013"::	uh, energy and fre- and frequency. And some are emphasizing  downward  and::	0
"me013"::	fast  things and  slow  things and -::	0
"me013"::	and  so forth. So.::	0
"me013"::	So there's a bunch of stuff to look at. But, uh, I think we're sorta gonna start off with what::	0
"me013"::	he, uh, came here with and branch out -::	0
"me013"::	branch out from there.::	0
"me013"::	And his advisor is::	0
"me013"::	here, too,  at the same time.  So,::	0
"me013"::	he'll be another  interesting source of  wisdom.::	0
"me006"::	Hmm.::	0
"me013"::	So.::	0
"me006"::	As - as we were talking about this I was thinking,::	0
"me013"::	Yeah.::	0
"me006"::	um,  whether there's a relationship between -::	0
"me006"::	um,::	0
"me006"::	between Michael's approach to, uh, some - some sort of optimal brain damage or optimal brain surgeon on the neural nets.::	0
"me006"::	So, like, if we have,::	0
"me026"::	Hmm .::	0
"me006"::	um -::	0
"me006"::	we have our - we have our RASTA features and -::	0
"me006"::	and  presumably  the neural nets are - are learning some sort of a nonlinear mapping,::	0
"me006"::	uh, from the - the - the features::	0
"me006"::	to - to this - this  probability  posterior space.::	0
"me013"::	Mm-hmm.::	0
"me006"::	Right? And,::	0
"me006"::	um -::	0
"me006"::	and each of the hidden units is learning some sort of - some sort of -::	0
"me006"::	some sort of  pattern.::	0
"me006"::	Right? And it could be, like -::	0
"me006"::	like these, um - these auditory patterns that Michael  is looking at.::	0
"me006"::	And then when you're looking at the -::	0
"me006"::	the, uh,  um,  the  best  features,::	0
"me006"::	you know, you can take out -  you can  do the - do this, uh, brain surgery by taking out,  um,  hidden  units that don't really help at all. And this is k- sorta like -::	0
"me013"::	Mm-hmm.::	0
"me013"::	Or the - or  features.::	0
"me013"::	Right?::	1
"me013"::	I mean, y- actually, you make me think a - a very important  point  here is that, um,::	0
"me006"::	Yeah.::	0
"me013"::	if we a- again try to look at how is this different from what we're already  doing,::	0
"me013"::	uh, there's a - a, uh -::	0
"me013"::	a nasty argument that could be made th- that it's - it's not different at - at  all,::	0
"me013"::	because, uh - if you ignore the - the selection part  -::	0
"me013"::	because we are going into a - a very powerful,::	0
"me013"::	uh, nonlinearity::	0
"me013"::	that, uh, in fact is  combining  over time and frequency,::	0
"me013"::	and is coming up with its own - you know,  better  than Gabor functions  - its, you know,  neural  net functions, its -   whatever it finds to be  best.::	0
"me006"::	Mm-hmm.::	0
"me026"::	@@::	0
"me013"::	Um, so you could argue that in fact it -  But I - I don't actually  believe  that argument because I know that, um,::	1
"me013"::	you can, uh -::	0
"me013"::	computing features is useful,::	0
"me013"::	even though::	0
"me013"::	in  principle  you haven't::	0
"me013"::	added  anything - in fact, you  subtracted  something, from the original  waveform  -::	0
"me013"::	You know, uh, if you've - you've processed it in some way you've typically lost  something  - some  information.  And so,::	0
"me013"::	you've  lost  information and yet it does  better::	0
"me013"::	with -  with features than it does with the waveform. So,::	0
"me013"::	uh, I - I know that i- sometimes it's useful to -::	0
"me013"::	to  constrain  things. So that's::	1
"me013"::	why it really seems like the  constraint  - in - in  all  this stuff it's the constraints that  are  actually what matters.::	1
"me013"::	Because if it wasn't  the constraints that mattered, then we would've  completely  solved this problem long  ago,  because long  ago  we already knew how to put waveforms into powerful statistical mechanisms.::	0
"me013"::	So.::	0
"mn007"::	Yeah. Well, if we had infinite processing power and  data,::	0
"me006"::	Right.::	0
"me013"::	Yeah-::	0
"me013"::	Uh,::	0
"mn007"::	I guess, using the waveform could -::	0
"me013"::	then it would work. Yeah, I agree.::	0
"me013"::	Yeah. There's the problem.::	0
"mn007"::	So, that's -::	0
"me013"::	Yeah. Then it would work. But - but, I mean, i- it's -::	0
"me013"::	With finite  of  those  things - I mean, uh, we - we  have  done experiments where we literally have put waveforms in and -::	0
"mn007"::	Mm-hmm.::	0
"me013"::	and - and, uh, we kept the number of parameters the same and so forth, and it used a  lot  of training data. And it - and it - it, uh -::	0
"me013"::	not infinite but a lot, and then compared to the number parameters - and it - it, uh - it just doesn't do nearly as well.::	0
"mn007"::	Mm-hmm.::	0
"me013"::	So,  anyway  the point is that you want to  suppress  -::	0
"me013"::	it's not just having the maximum information, you want to suppress,::	0
"me013"::	uh, the aspects of the input signal that are not helpful for -::	0
"me013"::	for the discrimination you're trying to make.::	0
"me013"::	So.::	0
"me013"::	So maybe just briefly, uh -::	0
"me006"::	Well, that sort of  segues  into  what - what  I'm  doing.::	1
"me013"::	Yeah.::	0
"me006"::	Um,::	1
"me006"::	so, uh, the big picture is k-::	0
"me006"::	um,  come up with a set of,::	0
"me006"::	uh, intermediate categories, then build intermediate category classifiers, then do  recognition,::	1
"me006"::	and, um, improve speech recognition in that way.::	0
"me006"::	Um, so right now I'm in - in the phase where  I'm looking at - at, um, deciding on a initial set of intermediate categories. And::	1
"me006"::	I'm looking  for data- data-driven::	0
"me006"::	methods that can help me find,::	0
"me006"::	um, a set of intermediate categories  of speech that, uh, will help me to discriminate  later down the line.::	1
"me006"::	And  one  of the ideas,  um, that was to take a - take a  neural  net -::	1
"me006"::	train - train an ordinary neural net::	1
"me006"::	to -::	0
"me006"::	uh, to learn the posterior probabilities of phones.::	1
"me006"::	And so,::	0
"me006"::	um, at the end of the day you have this neural net and it has hidden -::	0
"me006"::	hidden units. And each of these hidden units is -::	0
"me006"::	um, is learning some sort of pattern.::	0
"me006"::	And so, um, what - what  are  these patterns? I don't  know.::	0
"me018"::	Hmm.::	0
"me006"::	Um, and I'm gonna to try to -::	0
"me006"::	to look at those patterns::	0
"me006"::	to - to see,  um,  from those patterns -::	0
"me006"::	uh,  presumably  those are  important  patterns for discriminating between phone classes. And maybe -::	0
"me006"::	maybe some, uh, intermediate categories can come from::	0
"me006"::	just looking at the patterns of -::	0
"me006"::	um, that the neural net learns.::	0
"me013"::	Be- before you get on the next part l- let me just point out that s- there's - there's a - a pretty nice::	0
"me006"::	Yeah.::	0
"me013"::	relationship  between what  you're  talking about doing and what  you're  talking about doing there. Right? So,::	0
"me013"::	it seems to me that, you know, if you take away the - the -  the difference of this::	0
"me013"::	primary features,::	0
"me013"::	and, say, you use - as we had talked about maybe  doing  - you use P_- RASTA-P_L_P or something for the - the primary features,::	0
"me013"::	um, then this feature discovery,::	0
"me013"::	uh, uh, thing::	0
"me013"::	is just what  he's  talking about doing,  too,::	0
"me013"::	except that he's talking about doing them in order to discover  intermediate  categories  that correspond::	0
"me013"::	to these - uh, uh, what these::	0
"me013"::	sub-features are - are - are - are showing you.::	0
"me013"::	And, um,::	0
"me013"::	the  other  difference is that,::	0
"me013"::	um,::	0
"me013"::	he's doing this in a - in a  multi-band  setting,::	0
"me013"::	which means that he's  constraining  himself::	0
"me013"::	to look across  time  in some f- relatively limited, uh, uh, spectral extent. Right?::	0
"me013"::	And whereas in - in this case you're saying "let's just do it unconstrained".::	0
"me013"::	So they're - they're  really  pretty  related  and maybe they'll be - at  some  point where we'll::	0
"me013"::	see the - the connections a little better and::	0
"me026"::	Hmm.::	0
"me006"::	Mm-hmm.::	0
"me013"::	connect them.::	0
"me006"::	Um.::	0
"me006"::	Yeah, so - so that's the - that's the first part - uh, one - one of the ideas to get at some -::	0
"me006"::	some  patterns  of intermediate categories.::	0
"me006"::	Um,  the other one  was,::	1
"me006"::	um, to,::	0
"me006"::	uh, come up with a - a - a model -  um, a graphical model,::	0
"me006"::	that treats  the intermediate categories::	0
"me006"::	as  hidden  - hidden variables, latent variables, that we don't know anything  about,::	1
"me006"::	but that  through,::	1
"me006"::	um, s- statistical training and the E_M algorithm,  um, at the end of the  day,   we have, um -::	0
"me006"::	we have  learned  something about these - these latent, um - latent variables which  happen  to correspond to  intermediate categories.::	1
"me006"::	Um.::	0
"me006"::	Yeah, and so those are the - the two directions that I'm - I'm looking into right now.  And, uh,::	0
"me006"::	um -::	0
"me006"::	Yeah. I guess that's - that's it.::	0
"me013"::	O_K.::	0
"me013"::	Should we do our digits and::	0
"me013"::	get ou- get our treats?::	0
"me006"::	Oh, tea time?::	0
"me013"::	Yeah. It's kind of like, you know, the little rats with the little thing dropping down to  them.   We do the digits and then we get our treats.::	0
"me018"::	That's ri-::	0
"me006"::	Oops.::	0
"me018"::	O_K.::	0
"me013"::	O_K.::	0
