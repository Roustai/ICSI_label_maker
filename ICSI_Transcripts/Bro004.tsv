"me013"::	O_K.::	0
"me034"::	Oh, I don't -::	0
"me018"::	I think I'm zero.::	0
"me013"::	Wow!::	0
"fn002"::	Ah-::	0
"me034"::	Hello, hello, hello, hello.::	0
"me006"::	Wh- what causes the crash?::	0
"me013"::	Unprecedented.::	0
"me018"::	Did you fix something?::	0
"me034"::	Hello.::	0
"fn002"::	Five, five.::	0
"me006"::	Oh, maybe it's the turning - turning off and turning on of the mike, right?::	0
"me034"::	Hello, hello.::	0
"me013"::	Uh, you think that's  you?::	0
"me006"::	Yeah, O_K, mine's working.::	0
"me034"::	Aaa-aaa-aaa.  O_K. That's me.::	0
"me013"::	Oh.::	0
"me013"::	O_K.::	0
"me013"::	O_K. So, um I guess we are::	0
"me013"::	um  gonna do the digits at the end.::	0
"me013"::	Uh::	0
"mn007"::	Channel - channel three, yeah. O_K.::	0
"fn002"::	Mmm, channel five?::	0
"me034"::	Channel two.::	0
"fn002"::	Doesn't work?::	0
"me034"::	Two.::	0
"me013"::	Yeah,  that's  the  mike  number  there,::	0
"me013"::	uh::	0
"fn002"::	No?::	0
"me018"::	Is it written on her  sheet,  I believe.::	0
"mn007"::	Mike four.::	0
"me006"::	Watch this.   Yep, that's me.::	0
"me013"::	Uh, mike number five,::	0
"fn002"::	Ah,   era el cuatro.   Yeah.::	0
"me013"::	and::	0
"me018"::	But,  channel::	0
"me013"::	channel - channel four.::	0
"fn002"::	Yeah yeah yeah.::	0
"me013"::	This is  you.::	0
"fn002"::	O_K. I saw that.::	0
"fn002"::	Ah - yeah, it's O_K.::	0
"me013"::	Yeah.::	0
"me013"::	And I'm channel uh  two  I think,  or channel -::	0
"me034"::	Ooo.::	0
"me034"::	I think  I'm  channel two.::	0
"me013"::	Oh, I'm channel - must be channel one. Channel one?  Yes, O_K.::	0
"fn002"::	Channel -::	0
"me013"::	O_K. So uh::	0
"me013"::	I also copied uh the results that we all got in the mail I think from uh -::	0
"me013"::	from O_G_I and we'll go - go through them also.::	0
"me013"::	So where are we on -::	0
"me013"::	on uh::	0
"me013"::	our runs?::	0
"mn007"::	Uh so.  uh - We - So  As I was already said, we - we mainly focused on::	0
"mn007"::	uh four kind of features. The P_L_P, the P_L_P with J_RASTA, the M_S_G, and the M_F_C_C from the baseline Aurora.::	0
"me013"::	Excuse me.::	0
"fn002"::	I decided to talk about that.::	0
"me013"::	Mm-hmm.::	0
"mn007"::	Uh, and we focused for the - the test part on the English and the Italian.::	0
"mn007"::	Um. We've trained uh several neural networks on - so - on the T_I-digits English  and on the Italian data and also on the broad uh  English uh French and uh Spanish databases.::	0
"mn007"::	Mmm, so there's our result tables here, for the tandem approach, and um, actually what we - we  @@  observed is that if the network is trained on the task data it works pretty well.::	0
"me011"::	I can't get  back  far enough.::	0
"me034"::	Chicken on the grill.::	0
"me013"::	O_K. Our - our uh -::	0
"me013"::	There's a -  We're pausing for a photo -::	0
"me011"::	Sorry, guys.::	0
"me034"::	Try  that  corner.::	0
"me018"::	How about over th- from the front of the room?::	0
"me034"::	Yeah, it's longer.::	0
"me013"::	We're pausing for a photo opportunity here. Uh.::	0
"me013"::	Uh.::	0
"me013"::	So.::	0
"me006"::	Oh wait wait wait wait wait. Wait. Hold on. Hold on. Let me give you a black screen.::	0
"me034"::	Get out of the - Yeah.::	0
"me013"::	O_K.::	0
"me011"::	One more.::	0
"me013"::	He's facing  this  way.::	0
"me011"::	Because we  said  we were gonna  do  this and I  just  remembered.::	0
"me013"::	What?::	0
"me013"::	O_K, this - this would be a  good section for our silence detection.::	0
"me006"::	O_K.::	0
"me013"::	Um::	0
"me034"::	Mm-hmm.::	0
"me006"::	Musical chairs everybody!::	0
"me013"::	Oh.::	0
"me013"::	O_K.::	0
"me013"::	So um,::	0
"me013"::	you were saying::	0
"me013"::	about the training data -::	0
"mn007"::	Yeah, so if the network is trained on the task data um  tandem works pretty well. And uh actually we have uh, results are similar::	0
"me013"::	Yeah.::	0
"me018"::	Do you mean if it's trained  only  on -::	0
"mn007"::	Only on, yeah.::	0
"me018"::	On data from just that task, that language?::	0
"mn007"::	Just that task.::	0
"mn007"::	But actually we didn't train network on  uh both types of data I mean::	0
"mn007"::	uh  phonetically ba- phonetically balanced uh data and task data.::	0
"mn007"::	We only did  either  task - task data or  uh broad  data.::	0
"me018"::	Mmm.::	0
"me018"::	Mm-hmm.::	0
"mn007"::	Um::	0
"mn007"::	Yeah. So,::	0
"me013"::	So how - I mean -::	0
"me018"::	So what's th-::	0
"me013"::	clearly it's gonna be  good  then but the question is how much  worse is it::	0
"me013"::	if you have  broad  data?::	0
"me013"::	I mean,::	0
"me013"::	my assump-::	0
"me013"::	From what I saw from the earlier results, uh I guess last week,::	0
"me013"::	was that um,::	0
"me013"::	if you   trained  on one language and  tested  on another, say, that::	0
"me013"::	the results were - were relatively  poor.::	0
"mn007"::	Mmm. Yeah.::	0
"me013"::	But - but the question is if you  train  on one language::	0
"me013"::	but you have a  broad  coverage::	0
"me013"::	and then test in another,::	0
"me013"::	does that -  is that  improve  things::	0
"me013"::	i- c- in  comparison?::	0
"mn007"::	If we use the same language?::	0
"me013"::	No, no, no.::	0
"me013"::	Different lang-::	0
"me013"::	So::	0
"me013"::	um::	0
"me013"::	If you train on T_I-digits  and test on  Italian  digits,  you do poorly,::	0
"mn007"::	Mm-hmm.::	0
"me013"::	let's say.::	0
"me013"::	I don't have the  numbers  in front of me, so I'm just  imagining.::	0
"mn007"::	But -::	0
"mn007"::	Yeah but I did not uh do that. We -::	0
"me013"::	E-::	0
"me013"::	So, you didn't train on  TIMIT and test on -  on Italian digits, say?::	0
"mn007"::	No, we did four - four kind of - of testing, actually.::	0
"mn007"::	The first testing is  with task data -::	1
"mn007"::	So, with nets trained on task data.::	0
"mn007"::	So for Italian on the Italian speech  @@ .::	0
"mn007"::	The second test is trained on a single language::	0
"mn007"::	um with broad  database,  but the same language as the t- task data.::	0
"me013"::	O_K.::	0
"mn007"::	But for Italian we choose Spanish which  we assume is close to Italian.::	1
"mn007"::	The third test is by using, um::	1
"mn007"::	the three language database::	1
"mn007"::	and the fourth is::	0
"me013"::	W- which in - It has three languages. That's including the w-::	0
"me013"::	the -::	0
"me013"::	the -::	0
"mn007"::	This includes -::	0
"me013"::	the one that it's -::	0
"mn007"::	Yeah.::	0
"mn007"::	But  not::	0
"me018"::	In-::	0
"mn007"::	digits.  I mean it's -::	0
"me013"::	Right.::	0
"me018"::	The three languages::	0
"me018"::	is  not  digits, it's the broad::	1
"me018"::	data. O_K.::	1
"mn007"::	Yeah::	0
"mn007"::	And the fourth test is uh  excluding from these three languages the language  that is  the task language.::	1
"me013"::	Oh, O_K, yeah, so, that  is  what I wanted to know.::	0
"mn007"::	Yeah.::	0
"me013"::	I just wasn't  saying  it very well, I guess.::	0
"mn007"::	Uh, yeah.::	0
"mn007"::	So um  for uh T_I-digits for ins- example  uh when we go from T_I-digits training to  TIMIT training  uh we lose  uh around ten percent, uh.::	1
"me013"::	Relative.::	0
"mn007"::	The error rate increase u- of - of -::	0
"me013"::	Right.::	0
"mn007"::	of ten percent, relative.::	1
"mn007"::	So this is not so bad. And then when we jump to the multilingual data it's uh it become worse and, well::	1
"me013"::	Ab- about how much?::	0
"mn007"::	Around uh, let's say,  twenty perc- twenty percent further.  So.  Yeah.::	0
"me013"::	Twenty percent  further?::	0
"mn007"::	Twenty to - to thirty percent further. Yeah.::	1
"me018"::	And so, remind me, the  multilingual  stuff is  just  the broad data. Right?::	0
"mn007"::	Yeah.::	0
"me018"::	It's not the digits. So it's the combination of  two things there.::	0
"me018"::	It's  removing the  task specific  training and  it's adding other languages.::	0
"mn007"::	Yeah.::	0
"mn007"::	Yeah.::	0
"me018"::	O_K.::	0
"mn007"::	But the first step is al- already removing the task s- specific from - from -::	0
"me018"::	Already, right right right.::	0
"mn007"::	So. And we lose -::	0
"me018"::	So they were sort of  building   here? O_K?::	0
"mn007"::	Yeah.::	0
"mn007"::	Uh  So, basically when it's  trained  on the - the multilingual broad data::	0
"mn007"::	um or number - so, the - the  ratio of our error rates uh with the  baseline error rate is around  uh one point one.::	0
"mn007"::	So.::	0
"me013"::	Yes.::	0
"me013"::	And it's something like one point  three  of - of the::	0
"me013"::	uh -::	0
"me013"::	I- i- if you compare everything to the first case at the baseline,::	0
"me013"::	you get something like one point one::	0
"me013"::	for the - for the using the same language but a different task, and something like one point three::	0
"me013"::	for three - three languages::	0
"mn007"::	No no no.::	0
"me013"::	broad  stuff.::	0
"mn007"::	Uh same language we are at uh - for at English at O_ point eight.::	0
"mn007"::	So it improves,::	0
"mn007"::	compared to the baseline.::	0
"mn007"::	But -::	0
"mn007"::	So. Le-  let  me.::	0
"me013"::	I - I - I'm sorry. I - I - I meant something different by baseline::	0
"mn007"::	Tas- task data we are u-::	0
"mn007"::	Yeah.::	0
"me013"::	So let me - let me -::	0
"me013"::	Um,  so,  um -::	0
"mn007"::	Mmm.::	0
"me013"::	O_K, fine. Let's - let's use the conventional meaning of baseline. I - I -::	0
"mn007"::	Hmm.::	0
"me013"::	By baseline here I meant::	0
"me013"::	uh using the task specific data.::	0
"mn007"::	Oh yeah, the f-::	0
"mn007"::	Yeah, O_K. Yeah.::	0
"me013"::	But uh -::	0
"me013"::	uh, because that's what you were just doing with this ten percent.::	0
"me013"::	So I was just - I just trying to understand  that.  So if we call::	0
"mn007"::	Yeah. Sure.::	0
"me013"::	a factor of w- just  one,::	0
"me013"::	just normalized to  one,  the word error rate::	0
"mn007"::	Mmm.::	0
"me013"::	that you have  for using T_I-digits as - as::	0
"me013"::	training  and T_I-digits as  test,::	0
"me013"::	uh different  words,  I'm sure, but -::	0
"mn007"::	Mm-hmm.::	0
"me013"::	but uh, uh the same  task and so on.::	0
"me013"::	If we call that "one",  then what you're saying is::	0
"mn007"::	Mm-hmm.::	0
"me013"::	that the word error rate  for the same language but using  uh  different  training data than you're testing on, say TIMIT and so forth,::	0
"mn007"::	Mm-hmm.::	0
"me013"::	it's one point one.::	0
"mn007"::	Yeah, it's around one point one. Yeah.::	0
"me013"::	Right.::	0
"me013"::	And if it's -::	0
"me013"::	you  do  go to::	0
"me013"::	three languages  including  the English,::	0
"me013"::	it's something like one point  three.::	0
"mn007"::	Ye-::	0
"me013"::	That's  what you were just  saying,  I think.::	0
"mn007"::	Uh,  more  actually. If I -::	0
"me018"::	One point four?::	0
"mn007"::	Yeah.::	0
"me018"::	So, it's an  additional  thirty percent.::	0
"mn007"::	What would you say?  Around one point four yeah.::	0
"me013"::	O_K.::	0
"me013"::	And if you  exclude   English,::	0
"me013"::	from this combination, what's  that?::	0
"mn007"::	If we  exclude   English,   um  there is  not much difference with the  data  with  English.::	0
"me013"::	Aha!::	0
"mn007"::	So.::	0
"mn007"::	Yeah.::	0
"me013"::	That's  interesting.::	0
"me013"::	That's interesting.::	0
"me013"::	Do you see? Because -::	0
"mn007"::	Uh.::	0
"me013"::	Uh, so -::	0
"me013"::	No, that - that's important. So what - what it's saying here is just that "yes, there is a reduction::	0
"me013"::	in performance,::	0
"me013"::	when you don't  um::	0
"me013"::	have the s-::	0
"me013"::	when you don't have  um::	0
"me018"::	Task  data.::	0
"me013"::	Wait a minute, th- th- the -::	0
"mn007"::	Hmm.::	0
"me013"::	No, actually  it's  interesting.  So it's -::	0
"me013"::	So when you go to a different  task,  there's actually not so  different. It's when you went to these -::	0
"me013"::	So what's the difference between two and three?::	0
"me013"::	Between the one point one case and the one point four case? I'm confused.::	0
"me018"::	It's multilingual.::	0
"mn007"::	Yeah. The only difference it's - is that it's multilingual -::	0
"mn007"::	Um  Yeah.::	0
"me013"::	Cuz in both - in both -  both  of those cases, you don't have the same  task.::	0
"mn007"::	Yeah sure.::	0
"me013"::	So is - is the training data for the - for this one point four case -::	0
"me013"::	does it  include  the training data for the one point one case?::	0
"mn007"::	Uh yeah.::	0
"me006"::	Yeah, a fraction of it.::	0
"mn007"::	A part of it, yeah.::	0
"me013"::	How m- how much bigger is it?::	0
"mn007"::	Um::	0
"me006"::	Yeah, um.::	0
"mn007"::	It's two times, actually? Yeah.::	0
"mn007"::	Um.::	0
"mn007"::	The  English  data -::	0
"mn007"::	No, the  multilingual  databases are two times the  broad English  data.::	0
"mn007"::	We just wanted to keep this, w- well, not too  huge.  So.::	0
"me013"::	So it's  two  times,::	0
"me013"::	but it  includes  the - but it  includes  the broad English data.::	0
"mn007"::	I think so.  Do you  -::	0
"mn007"::	Uh,::	0
"mn007"::	Yeah.::	0
"me013"::	And the broad English data is what you got this one point one  with. So that's TIMIT basically right?::	0
"me006"::	Mm-hmm.::	0
"mn007"::	Yeah.::	0
"me013"::	So it's band-limited TIMIT.::	0
"me006"::	Mm-hmm.::	0
"mn007"::	Mm-hmm.::	0
"me013"::	This is all::	0
"mn007"::	Yeah.::	0
"me006"::	Downs-::	0
"me013"::	eight kilohertz sampling.::	0
"me006"::	Right.::	0
"me013"::	So you have band-limited TIMIT,  gave you uh::	0
"me013"::	almost  as good as a result as using T_I- digits::	0
"me013"::	on a T_I-digits  test.::	0
"me013"::	O_K?::	0
"mn007"::	Hmm?::	0
"me013"::	Um  and  um::	0
"me013"::	But,::	0
"me013"::	when you add in more training data but keep the neural net the same size,::	0
"me013"::	it  um performs worse on the T_I-digits.::	0
"me013"::	O_K, now all of this is -::	0
"me013"::	This is  noisy   T_I-digits, I assume?::	0
"mn007"::	Yep.::	0
"me013"::	Both training and test?::	0
"me013"::	Yeah.  O_K.::	0
"me013"::	Um::	0
"me013"::	O_K.::	0
"me013"::	Well.::	0
"me013"::	We - we - we may just need to uh -::	0
"me013"::	So I mean it's interesting that h- going to a different -::	0
"me013"::	different  task  didn't seem to  hurt  us that much, and going to a different  language::	0
"me013"::	um::	0
"me013"::	It doesn't seem to matter -::	0
"me013"::	The difference between  three  and  four  is not particularly great, so that means that::	0
"me013"::	whether you have the  language  in or  not  is not such a big deal.::	0
"mn007"::	Mmm.::	0
"me013"::	It sounds like um::	0
"me013"::	uh::	0
"me013"::	we  may  need to have more::	0
"me013"::	of uh things that are similar to a target language or - I mean.::	0
"me013"::	You have the  same  number of parameters in the  neural  net, you haven't increased the size of the neural net,::	0
"me013"::	and maybe there's just -::	0
"me013"::	just not enough::	0
"me013"::	complexity to it to represent::	0
"me013"::	the variab- increased variability in the -::	0
"me013"::	in the training set.::	0
"me013"::	That - that could be.::	0
"me013"::	Um::	0
"me013"::	So, what about -::	0
"me013"::	So  these  are results with::	0
"me013"::	uh th-::	0
"me013"::	that you're describing now, that::	0
"me013"::	they are pretty similar for the different features or -::	0
"me013"::	or uh -::	0
"mn007"::	Uh, let me check. Uh.::	0
"mn007"::	So. This was for the P_L_P,::	1
"me013"::	Yeah.::	0
"mn007"::	Um. The - Yeah. For the P_L_P with J_RASTA the -::	1
"me013"::	Yeah.::	0
"mn007"::	the - we -::	1
"mn007"::	This is quite the same  tendency,::	1
"mn007"::	with a slight increase of the error rate,::	0
"mn007"::	uh if we go to - to TIMIT.::	1
"mn007"::	And then it's - it gets worse with the multilingual.::	1
"mn007"::	Um.::	0
"mn007"::	Yeah. There - there is a difference actually with - b- between P_L_P and J_RASTA is that::	1
"mn007"::	J_RASTA  seems to  perform better with the highly mismatched  condition::	0
"mn007"::	but  slightly   - slightly worse  for the well matched condition.::	1
"mn007"::	Mmm.::	0
"me013"::	I have a suggestion, actually, even though it'll delay us slightly, would - would you mind::	0
"me013"::	running into the other room and making::	0
"me013"::	copies of this?  Cuz we're all sort of -::	0
"mn007"::	Yeah, yeah.::	0
"me013"::	If we c- if we could look at it, while we're talking, I think  it 'd be uh -::	0
"mn007"::	O_K.::	0
"me013"::	Uh, I'll - I'll sing a song or dance or something while you  do it, too.::	0
"me006"::	Alright.::	0
"me018"::	So um - Go ahead. Ah, while you're gone I'll ask s- some of my questions.::	0
"me013"::	Yeah.::	0
"me018"::	Um.::	0
"me013"::	Yeah.::	0
"me013"::	Uh,::	0
"me013"::	this way and just slightly to the left, yeah.::	0
"me018"::	The um -::	0
"me018"::	What was - Was this number  forty or - It was roughly the same as this one,  he said?  When you had the  two  language versus the  three  language?::	0
"me013"::	Um.::	0
"me013"::	That's what he was  saying.::	0
"me018"::	That's where he removed English, right?::	0
"me006"::	Yeah.::	0
"me013"::	Right.::	0
"me006"::	It sometimes, actually, depends on what features you're using.::	0
"me013"::	Yeah.::	0
"me013"::	But - but i- it sounds like -::	0
"me006"::	Um, but -::	0
"me013"::	I mean.  That's  interesting because::	0
"me013"::	it - it  seems  like what it's saying is  not  so much that you got  hurt::	0
"me013"::	uh because  you::	0
"me013"::	uh didn't have so much representation of  English,::	0
"me013"::	because in the  other  case you don't get hurt  any   more,   at least when::	0
"me013"::	it seemed like uh it - it might simply be a case that you  have  something that is just much more  diverse,::	0
"me018"::	Mm-hmm.::	0
"me013"::	but you have the same number of parameters  representing  it.::	0
"me018"::	Mm-hmm.  I wonder - were um all three of these nets  using the same output? This  multi-language    uh labeling?::	0
"me006"::	He -::	0
"me006"::	Mm-hmm.::	0
"me006"::	He was using uh sixty-four phonemes from  SAMPA.::	0
"me018"::	O_K, O_K.::	0
"me006"::	Yeah.::	0
"me018"::	So  this  would -::	0
"me018"::	From  this  you would say, "well, it doesn't really  matter  if we put Finnish::	0
"me018"::	into  the training of the neural net,::	0
"me018"::	if there's  gonna be,::	0
"me018"::	you know, Finnish in the test data." Right?::	0
"me013"::	Well, it's - it sounds -::	0
"me013"::	I mean, we have to be  careful,  cuz we haven't gotten a  good  result yet.::	0
"me018"::	Yeah.::	0
"me013"::	And comparing different  bad  results can be  tricky.::	0
"me018"::	Hmm.::	0
"me013"::	But I - I - I -::	0
"me013"::	I think it  does  suggest that it's not so much uh::	0
"me013"::	uh cross   language  as cross  type  of  speech.::	0
"me018"::	Mm-hmm.::	0
"me013"::	It's - it's um -::	0
"me013"::	But we  did  - Oh yeah, the other thing I was  asking  him, though, is that I  think  that in the case -::	0
"me013"::	Yeah, you - you  do  have to be careful because of com- compounded results. I think we got some earlier results::	0
"me013"::	in which you  trained  on one language and  tested  on  another  and you didn't have::	0
"me013"::	three,  but you just had  one   language. So you trained on::	0
"me013"::	one type of digits and  tested  on  another.  Didn-::	0
"me013"::	Wasn't there something of that? Where you,::	0
"me013"::	say, trained on Spanish and tested on - on T_I-digits, or the other way around?::	0
"me013"::	Something like that?::	0
"fn002"::	No.::	0
"me013"::	I thought there was  something  like that,::	0
"me013"::	that he showed me  last week.::	0
"me013"::	We'll have to wait till we get -::	0
"me018"::	Yeah, that would be interesting.::	0
"me013"::	Um,::	0
"me013"::	This  may  have been what I was asking before, Stephane, but -::	0
"me013"::	but, um, wasn't there  something  that you did,::	0
"me013"::	where you  trained   on  one  language and  tested  on  another?::	0
"me013"::	I mean no - no  mixture  but just -::	0
"me006"::	I'll get it for you.::	0
"mn007"::	Uh, no, no.::	0
"me013"::	We've never just trained on one lang-::	0
"mn007"::	Training on a  single  language, you mean, and  testing  on the  other  one?::	0
"me013"::	Yeah.::	0
"fn002"::	Not yet.::	0
"mn007"::	Uh, no.::	0
"mn007"::	So the only  task that's similar to this is the training on  two  languages, and::	0
"me013"::	But we've  done  a bunch of things where we just trained on one  language.  Right?::	0
"mn007"::	that -::	0
"me013"::	I mean, you haven't - you haven't done  all  your tests on multiple  languages.::	0
"mn007"::	Uh,::	0
"mn007"::	No.::	0
"mn007"::	Either thi- this is test with  uh the  same  language  but from the  broad  data, or it's test with  uh  different  languages::	0
"mn007"::	also  from the broad data,  excluding  the -::	0
"fn002"::	The  early  experiment that -::	0
"mn007"::	So, it's - it's three or - three and four.::	0
"me018"::	Did you do different languages from digits?::	0
"mn007"::	Uh. No.::	0
"mn007"::	You mean::	0
"mn007"::	training digits::	0
"mn007"::	on one language and using the net::	0
"mn007"::	to recognize on the other?::	0
"me018"::	Digits on another language?::	0
"mn007"::	No.::	0
"me013"::	See, I  thought  you  showed  me something like that last  week.::	0
"me013"::	You had a - you had a little -::	0
"mn007"::	Uh,::	0
"mn007"::	No, I don't  think  so.::	0
"me013"::	Um::	0
"me013"::	What -::	0
"me034"::	These numbers are uh::	0
"me034"::	ratio to baseline?::	0
"me013"::	So, I mean wha- what's the - This - this chart - this table that we're looking at  is um,::	0
"mn007"::	So.::	0
"me013"::	show- is all  testing  for  T_I -digits, or - ?::	0
"me006"::	Bigger is worse.  This is error rate, I think.::	0
"mn007"::	So you have uh basically two  uh parts. The upper part is for T_I-digits::	0
"me034"::	Ratio.::	0
"me006"::	No.  No.::	0
"me006"::	Yeah, yeah, yeah.::	0
"mn007"::	and it's divided in three  rows  of four - four rows each.::	0
"me006"::	Mm-hmm.::	0
"me013"::	Yeah.::	0
"mn007"::	And the first four rows is well-matched, then the s- the second group of four rows is mismatched, and::	0
"mn007"::	finally highly mismatched.::	0
"mn007"::	And then the lower part is for Italian and it's the same -::	0
"mn007"::	the same thing.::	0
"me018"::	So, so the upper part is  training   T_I-digits?::	0
"mn007"::	So.::	0
"mn007"::	It's - it's the H_T_K results, I mean. So it's::	0
"mn007"::	H_T_K training testings::	0
"me018"::	Ah.::	0
"mn007"::	with different kind of features::	0
"mn007"::	and what appears in the::	0
"mn007"::	uh left column is  the networks that are used for doing this.::	0
"me013"::	Hmm.::	0
"mn007"::	So.::	0
"mn007"::	Uh::	0
"mn007"::	Yeah.::	0
"me013"::	Well,::	0
"me013"::	What was is that i-::	0
"me013"::	What was it that you had::	0
"me013"::	done  last week when you showed - Do you remember?::	0
"me013"::	Wh- when you showed me  the - your table last week?::	0
"mn007"::	It- It was part of these results.::	0
"mn007"::	Mmm.::	0
"mn007"::	Mmm.::	0
"me018"::	So where is the baseline  for the T_I-digits  located in here?::	0
"mn007"::	You mean the H_T_K Aurora baseline?::	0
"me018"::	Yeah.::	0
"mn007"::	It's uh the one hundred number.::	0
"mn007"::	It's, well, all these numbers are the ratio::	0
"me018"::	Ah!::	0
"mn007"::	with respect to the baseline.::	0
"me018"::	Ah, O_K, O_K.::	0
"me013"::	So this is word - word error rate, so a  high  number is  bad.::	0
"mn007"::	Yeah, this is  a word error rate ratio.::	0
"fn002"::	Yeah.::	0
"mn007"::	Yeah.::	0
"me018"::	O_K, I see.::	0
"mn007"::	So,::	0
"mn007"::	seventy point two means that::	0
"mn007"::	we reduced the error rate uh by thirty - thirty percent. So.::	0
"me018"::	O_K, O_K, gotcha.::	0
"me013"::	O_K,::	0
"mn007"::	Hmm.::	0
"me013"::	so if we take uh::	0
"me013"::	um::	0
"me013"::	let's see::	0
"me013"::	P_L_P::	0
"me013"::	uh with on-line  normalization and  delta-del- so that's this thing you have circled here::	0
"me013"::	in the second column,::	0
"mn007"::	Yeah.::	0
"me013"::	um::	0
"me013"::	and "multi-English" refers to what?::	0
"mn007"::	To TIMIT.::	0
"mn007"::	Mmm.::	0
"mn007"::	Then you have  uh M_F,  M_S and M_E which are for French, Spanish and English.::	0
"mn007"::	And,  yeah.::	0
"mn007"::	Actually I -  I uh forgot to say that  the multilingual net are trained::	0
"mn007"::	on  uh  features without the s- derivatives::	0
"mn007"::	uh but with  increased frame numbers. Mmm.::	0
"mn007"::	And we can - we can see on the first line of the table that it - it -::	0
"mn007"::	it's  slightly   - slightly worse when we don't use delta but it's not -::	0
"mn007"::	not  that  much.::	0
"me013"::	Right.::	0
"me013"::	So w- w-::	0
"me013"::	So, I'm sorry. I missed that. What's M_F, M_S and M_E?::	0
"me018"::	Multi-French, Multi-Spanish::	0
"mn007"::	So. Multi-French, Multi-Spanish, and Multi-English.::	0
"me013"::	Uh O_K. So, it's  uh   broader  vocabulary.::	0
"mn007"::	Yeah.::	0
"me013"::	Then -::	0
"me013"::	And -::	0
"me013"::	O_K so I  think  what I'm -::	0
"me013"::	what I saw in your smaller chart that I was thinking of was -::	0
"me013"::	was::	0
"me013"::	there were  some  numbers I saw, I think, that  included  these multiple languages::	0
"me013"::	and it -::	0
"me013"::	and I was seeing::	0
"me013"::	that it got  worse.::	0
"me013"::	I - I think that was all it was. You had some very limited results that - at that point::	0
"mn007"::	Yeah.::	0
"me013"::	which showed  having in these - these other languages. In fact it might have been just this last category,::	0
"me013"::	having two languages broad::	0
"me013"::	that were -::	0
"me013"::	where - where English was  removed.::	0
"me013"::	So that was cross language::	0
"me013"::	and the - and the result was quite  poor.::	0
"me013"::	What I -::	0
"me013"::	we  hadn't  seen yet was that if you added  in  the English, it's  still  poor.::	0
"mn007"::	Yeah.  Still poor.::	0
"me013"::	Uh::	1
"me013"::	Um now, what's the noise condition::	0
"me013"::	um  of the training data -::	1
"me013"::	Well, I think this is what you were explaining. The  noise  condition is the same -::	1
"me013"::	It's the same uh Aurora noises::	0
"mn007"::	Yeah.::	0
"me013"::	uh, in all these cases::	0
"mn007"::	Yeah.::	0
"me013"::	for the training.::	0
"me013"::	So there's not a  statistical - sta- a strong st-::	1
"me013"::	statistically different::	0
"me013"::	noise characteristic between::	0
"mn007"::	No these are the s- s- s- same noises, yeah.::	1
"me013"::	uh the training and test::	0
"me013"::	and yet we're seeing  some  kind of effect -::	1
"mn007"::	At least - at least for the first -::	1
"mn007"::	for the well-matched, yeah.::	1
"me006"::	Well matched condition.::	0
"me013"::	Right.::	0
"me013"::	So there's  some  kind of a - a - an effect from having these - uh this broader coverage::	0
"me013"::	um::	0
"me013"::	Now I  guess  what we should try doing with this is try::	0
"me013"::	testing these on u- this same sort of thing on -::	0
"me013"::	you probably  must  have this::	0
"me013"::	lined  up to  do.  To try the same t-::	0
"me013"::	with the  exact  same  training,  do testing on::	0
"me013"::	the other  languages.::	0
"mn007"::	Mmm.::	0
"me013"::	On - on um -::	0
"me013"::	So.::	0
"me013"::	Um, oh I well,  wait  a minute. You  have  this  here,  for the  Italian.   That's  right.  O_K, so,::	0
"mn007"::	Yeah.::	0
"mn007"::	Yeah, so for the Italian the results are  uh  stranger um::	1
"me013"::	So.::	0
"mn007"::	Mmm.::	0
"mn007"::	So what appears is that perhaps Spanish is::	1
"mn007"::	not very close to Italian::	1
"mn007"::	because uh, well,::	1
"mn007"::	when using the - the network trained only on Spanish it's -::	0
"mn007"::	the error rate is::	0
"mn007"::	almost uh twice::	0
"mn007"::	the baseline error rate.::	1
"me013"::	Mm-hmm.::	0
"mn007"::	Mmm.  Uh.::	0
"me013"::	Well, I mean, let's see.::	0
"me013"::	Is there any difference in -::	0
"me013"::	So it's in  the uh -::	0
"me013"::	So you're saying that::	0
"me013"::	when you  train  on English::	0
"me013"::	and  uh  and - and  test  on -::	0
"mn007"::	Yeah.::	0
"me013"::	No, you  don't  have training on English testing -::	0
"mn007"::	There - there is -  another  difference, is that the noise - the noises are different. Well,::	1
"me013"::	In - in  what?::	1
"mn007"::	For - for the Italian part I mean the::	1
"mn007"::	uh  the um::	0
"mn007"::	networks are trained with noise from  Aurora - T_I- digits,  mmm.::	1
"fn002"::	Aurora-two.::	0
"mn007"::	Yeah.::	0
"me013"::	And the noise is different  in th-::	0
"mn007"::	And perhaps the noise are::	1
"mn007"::	quite different from the noises  in the speech that  Italian .::	1
"mn007"::	And -::	0
"me013"::	Do we have any um::	0
"me013"::	test sets::	0
"me013"::	uh in  any other language that um::	0
"me013"::	have the  same  noise as in  the Aurora?::	0
"fn002"::	Mmm, no.::	0
"mn007"::	No.::	1
"me018"::	Can I ask something real quick?::	0
"me018"::	In - in the  upper  part -::	0
"me018"::	in the  English   stuff,::	0
"me018"::	it  looks  like the  very   best  number is sixty point nine?::	0
"me018"::	and that's in the uh -::	0
"me018"::	the third  section in the upper part under P_L_P J_RASTA,::	0
"me018"::	sort of the middle column?::	0
"mn007"::	Yeah.::	0
"me018"::	I- is  that   a  noisy  condition?::	0
"mn007"::	Yeah.::	0
"me018"::	So that's  matched  training? Is  that  what that  is?::	0
"mn007"::	It's - no, the third part, so it's uh  highly mismatched.::	1
"mn007"::	So. Training and  test noise are different.::	0
"me018"::	So - why do you get your  best   number::	0
"me018"::	in -::	0
"me018"::	Wouldn't you get your  best  number in the  clean  case?::	0
"me034"::	Well, it's relative to the::	0
"me034"::	um::	0
"me034"::	baseline mismatching::	0
"mn007"::	Yeah.::	0
"me018"::	Ah, O_K so these are not -::	0
"mn007"::	Yeah.::	1
"me018"::	O_K,  alright, I see.::	0
"me034"::	Yeah.::	0
"mn007"::	Yeah.::	1
"me018"::	O_K.::	0
"me018"::	And then - so, in the - in the um -::	0
"me018"::	in the   non-mismatched   clean  case,::	0
"me018"::	your best one was under M_F_C_C?::	0
"me018"::	That sixty-one point four?::	0
"mn007"::	Yeah.  But it's not a  clean  case. It's  a noisy case but::	1
"mn007"::	uh training and test noises are the same.::	0
"me018"::	Oh! So this upper third?::	0
"mn007"::	So - Yeah.::	0
"me018"::	Uh that's still noisy?::	0
"mn007"::	Yeah.::	0
"me018"::	Ah, O_K.::	0
"mn007"::	So it's always noisy basically,  and,  well, the -::	1
"me018"::	Mm-hmm.::	0
"me018"::	I see.::	0
"mn007"::	Mmm.::	0
"me013"::	O_K?::	0
"me013"::	Um::	0
"me013"::	So uh, I think this will take some  looking at, thinking about. But,::	0
"me013"::	what is uh -::	0
"me013"::	what is  currently  running, that's -::	0
"me013"::	uh, i- that -::	0
"me013"::	just filling in the holes here or - or - ?::	0
"mn007"::	Uh, no we don't plan to fill the holes but::	1
"me013"::	pretty much?::	0
"me013"::	O_K.::	0
"mn007"::	actually there is something important,::	1
"mn007"::	is that  um we made a lot of assumption concerning the on-line normalization::	1
"mn007"::	and we just noticed::	1
"mn007"::	uh recently that::	0
"mn007"::	uh the  approach that we were using::	0
"mn007"::	was not  uh::	0
"mn007"::	leading to very good results::	0
"mn007"::	when we  used the straight features to H_T_K.::	1
"mn007"::	Um::	0
"mn007"::	Mmm. So basically d-::	0
"mn007"::	if you look at the - at the left of the table,::	0
"mn007"::	the first uh row,::	0
"mn007"::	with eighty-six, one hundred, and forty-three and seventy-five,::	0
"mn007"::	these are the results we obtained for Italian::	0
"mn007"::	uh with  straight  mmm, P_L_P features::	0
"mn007"::	using on-line normalization.::	0
"me013"::	Mm-hmm.::	0
"mn007"::	Mmm.::	0
"mn007"::	And the, mmm - what's  in the table, just  at the left of the P_L_P twelve  on-line normalization column,::	0
"mn007"::	so, the numbers seventy-nine, fifty-four and  uh forty-two::	1
"mn007"::	are the results obtained by uh Pratibha with  uh his on-line normalization - uh  her  on-line normalization approach.::	1
"me018"::	Where is that? seventy-nine, fifty::	0
"fn002"::	Fifty-one? This -::	0
"me013"::	Uh, it's just sort of sitting right on the uh - the column line.::	0
"mn007"::	So.::	0
"me013"::	Uh.::	0
"me018"::	Oh I see, O_K.::	0
"mn007"::	Just - uh::	0
"me013"::	Yeah.::	0
"mn007"::	Yeah.::	0
"mn007"::	So these are the results of  O_G_I with  on-line normalization and straight features to H_T_K.::	0
"mn007"::	And the previous result, eighty-six and so on,::	1
"me013"::	Yes.::	0
"mn007"::	are with our  features straight to H_T_K. So::	1
"me013"::	Yes.::	0
"mn007"::	what we see that - is - there is that um::	0
"mn007"::	uh the way we were doing this was not correct, but  still::	1
"mn007"::	the networks  are very good.::	0
"mn007"::	When we use the networks::	1
"mn007"::	our number are better that::	0
"fn002"::	We improve.::	0
"mn007"::	uh Pratibha results.::	1
"me013"::	So, do you know what was  wrong  with the on-line normalization, or - ?::	0
"mn007"::	Yeah. There were diff- there were different things and::	1
"mn007"::	basically,  the first thing is the mmm,::	0
"mn007"::	alpha uh  value. So, the recursion  uh  part.::	1
"mn007"::	um,::	1
"mn007"::	I used point five percent,  which was the default value in the -::	0
"mn007"::	in the programs here.::	1
"mn007"::	And Pratibha used  five  percent.::	1
"mn007"::	So it adapts more  quickly::	0
"me013"::	Uh-huh.::	0
"me013"::	Yes. Yeah.::	0
"mn007"::	Um, but, yeah. I assume that this was not important because::	1
"mn007"::	uh previous results from - from Dan and - show that basically::	0
"mn007"::	the  both - both values g- give the same - same  uh results.::	1
"mn007"::	It was true on uh  T_I-digits but it's not true on Italian.::	1
"me013"::	Mm-hmm.::	0
"mn007"::	Uh, second thing is the initialization of the  stuff. Actually,::	1
"mn007"::	uh what we were doing is to start the recursion from the beginning of the  utterance.::	1
"mn007"::	And using initial values that are the global mean and variances  measured across the  whole  database.::	1
"me013"::	Right.  Right.::	0
"mn007"::	And Pratibha did something different is that he - uh she initialed the um values of the mean and variance::	1
"mn007"::	by computing  this on the  twenty-five first frames of each utterance.::	1
"mn007"::	Mmm. There were other minor differences, the fact that::	1
"mn007"::	she used fifteen  dissities  instead s- instead of thirteen,::	0
"mn007"::	and that she used C_zero instead of log energy.::	0
"mn007"::	Uh, but the main differences concerns the recursion.::	0
"mn007"::	So.::	1
"mn007"::	Uh, I changed the code::	1
"mn007"::	uh and now we have a baseline that's similar to the O_G_I baseline.::	1
"me013"::	O_K.::	0
"mn007"::	We - It - it's  slightly    uh different because::	0
"mn007"::	I don't exactly initialize the same way she does.::	0
"mn007"::	Actually I start,::	0
"mn007"::	mmm, I don't wait to a fifteen - twenty-five - twenty-five frames::	0
"mn007"::	before computing a mean and the variance::	0
"mn007"::	to e- to - to start the recursion.::	0
"me034"::	Mm-hmm.::	0
"me013"::	Yeah.::	0
"mn007"::	I - I use the on-line scheme::	0
"mn007"::	and only start the re- recursion after the twenty-five -  twenty-fifth frame.::	0
"mn007"::	But, well it's similar.::	0
"mn007"::	So::	0
"mn007"::	uh I retrained  the networks with  these - well, the - the - the networks are retaining with these new  features.::	1
"me013"::	Mm-hmm.::	0
"mn007"::	And, yeah.::	0
"me013"::	O_K.::	0
"mn007"::	So basically what I expect is that::	0
"mn007"::	these numbers will a little bit go down but::	0
"mn007"::	perhaps not - not so much::	0
"mn007"::	because  I think the neural networks learn perhaps::	0
"me013"::	Right.::	0
"mn007"::	to -::	0
"mn007"::	even if the features are not  normalized. It - it will learn how to normalize and -::	0
"me013"::	Right.::	0
"me013"::	O_K, but I think that::	1
"mn007"::	Mmm.::	0
"me013"::	given the pressure of time we probably want to draw -::	0
"me013"::	because of  that    especially,  we wanna draw some  conclusions  from this, do some  reductions::	1
"me013"::	in what we're  looking  at,::	0
"mn007"::	Yeah.::	0
"me013"::	and make some  strong  decisions for what we're gonna do  testing  on before next  week.::	1
"me013"::	So do you - are you - w-::	0
"mn007"::	Yeah   I'd  -::	0
"me013"::	did you have something going on, on the side, with uh multi-band  or - on - on this, or - ?::	0
"mn007"::	No, I - we plan to start this uh so, act- actually we have discussed uh::	1
"mn007"::	@@  um, these -::	1
"mn007"::	what we could do::	0
"mn007"::	more as a - as a research and -::	1
"mn007"::	and  we were thinking perhaps that::	0
"mn007"::	uh  the way we use the tandem is not -::	1
"mn007"::	Uh, well, there is basically perhaps a flaw in the - in the - the stuff because::	0
"mn007"::	we  trained the networks -::	1
"mn007"::	If we trained the networks on the - on::	1
"mn007"::	a language and a t- or a specific  task,::	1
"me013"::	Mm-hmm.::	0
"mn007"::	um, what we ask is - to the network - is to put the bound- the decision boundaries somewhere in the space.::	1
"mn007"::	And uh  mmm and ask the network to put one,::	1
"me013"::	Mmm.::	0
"mn007"::	at one side of the - for - for a particular phoneme at one side of the boundary - decision boundary and one for another phoneme at the other side.::	1
"mn007"::	And  so there is kind of reduction of the information there that's not correct because if we change task::	1
"mn007"::	and if the phonemes are not in the same context in the new task,::	0
"mn007"::	obviously the  decision boundaries are not -::	0
"mn007"::	should not be at the same  place.::	1
"mn007"::	But the way the feature gives -::	1
"me013"::	I di-::	0
"mn007"::	The - the way the network gives the features is that it reduce completely the -::	0
"mn007"::	it removes completely the information -::	0
"mn007"::	a lot of information from the - the features::	0
"mn007"::	by uh::	0
"mn007"::	uh::	0
"mn007"::	placing the decision boundaries at  optimal places for::	0
"mn007"::	one kind of  data  but::	1
"mn007"::	this is not the case for another kind of data.::	0
"me013"::	It's  a  trade-off,  right?  Any-  anyway  go  ahead.::	0
"mn007"::	So -::	0
"mn007"::	Yeah. So uh what we were thinking about is perhaps::	1
"mn007"::	um one way::	0
"mn007"::	to solve this problem is increase the number of  outputs of the neural networks.::	1
"mn007"::	Doing something like, um::	0
"mn007"::	um phonemes within context and,::	0
"mn007"::	well, basically context dependent phonemes.::	1
"me013"::	Maybe. I mean, I - I think::	0
"me013"::	you could make  the same argument,::	0
"me013"::	it'd be just as legitimate,::	0
"me013"::	for hybrid systems::	0
"me013"::	as well.::	0
"mn007"::	Yeah but, we know that -::	0
"me013"::	Right.::	0
"me013"::	And in fact,::	0
"me013"::	th- things get  better  with context  dependent    versions.::	0
"me013"::	Right?::	0
"mn007"::	Ye- yeah but here it's something different. We want to have features uh well,::	0
"me013"::	Yeah.::	0
"mn007"::	um.::	0
"me013"::	Yeah,  but it's  still  true::	0
"me013"::	that what you're doing::	0
"me013"::	is  you're ignoring -::	0
"me013"::	you're - you're coming up with something to represent,::	0
"me013"::	whether it's a distribution,::	0
"me013"::	probability distribution or  features,::	0
"me013"::	you're coming up with a set of variables::	0
"me013"::	that are representing::	0
"me013"::	uh,::	0
"me013"::	things that vary w- over context.::	0
"mn007"::	Mm-hmm.::	0
"me013"::	Uh, and you're  putting it all  together,::	0
"me013"::	ignoring  the differences in context.::	0
"me013"::	That - that's true::	0
"me013"::	for the hybrid system,::	0
"me013"::	it's true for a tandem system.::	0
"me013"::	So, for that reason, when you - in -::	0
"me013"::	in - in a  hybrid  system,::	0
"me013"::	when you incorporate  context  one way or another,::	0
"me013"::	you  do  get better  scores.::	0
"mn007"::	Yeah.::	0
"me013"::	O_K?  But I - it's - it's a big  deal::	0
"me013"::	to  get  that.::	0
"me013"::	I - I'm - I'm sort of -::	0
"me013"::	And  once  you - the other thing is that  once  you represent - start representing more and more  context::	1
"me013"::	it  is   uh::	0
"me013"::	much more::	0
"me013"::	um  specific::	0
"me013"::	to a particular task in  language.::	1
"me013"::	So um::	0
"me013"::	Uh, the -::	0
"me013"::	the  acoustics   associated  with  uh::	0
"me013"::	a particular context, for instance you may have some kinds of contexts that will  never  occur::	1
"me013"::	in one language and will occur  frequently  in the  other,  so the qu- the issue of getting enough  training::	1
"me013"::	for a particular kind of  context  becomes harder.::	1
"me013"::	We already actually don't have a huge amount of training data::	1
"me013"::	um::	0
"mn007"::	Yeah, but -::	0
"mn007"::	mmm,::	0
"mn007"::	I mean,  the - the way we - we do it now is that we have a neural network and::	1
"mn007"::	basically::	0
"mn007"::	the net- network is trained almost to give binary decisions.::	1
"me013"::	Right.::	0
"mn007"::	And  uh -::	0
"mn007"::	binary decisions about  phonemes.  Nnn -::	0
"mn007"::	Uh::	0
"me013"::	Almost.::	0
"me013"::	But I mean it - it - it  does  give a  distribution.::	0
"mn007"::	It's -::	0
"mn007"::	Yeah.::	0
"me013"::	It's - and -::	0
"me013"::	and  it  is  true that if there's two phones that are very  similar,::	0
"me013"::	that  uh  the -::	0
"me013"::	i- it may prefer one but it will::	0
"me013"::	give a reasonably high value to the other, too.::	0
"mn007"::	Yeah.::	0
"mn007"::	Yeah, sure but  uh::	0
"mn007"::	So basically it's almost binary decisions and::	0
"mn007"::	um the idea of using more  classes is::	0
"mn007"::	to  get something that's  less binary decisions.::	0
"me013"::	Oh no, but it would  still  be even  more  of a binary decision.::	1
"me013"::	It - it'd be even  more  of one.::	0
"me013"::	Because then you would say::	0
"mn007"::	But - yeah, but -::	0
"me013"::	that in - that this phone in  this  context is a  one,::	0
"me013"::	but the  same  phone in a slightly  different  context is a  zero.::	0
"me013"::	That would be even - even  more  distinct of a binary decision.::	1
"me013"::	I  actually would have thought you'd wanna go the  other  way and have  fewer  classes.::	1
"mn007"::	Yeah, but if -::	0
"me013"::	Uh, I mean for instance,::	0
"me013"::	the - the thing I was arguing for  before,  but::	0
"mn007"::	Mmm.::	0
"me013"::	again which I don't think we have time to  try,::	0
"me013"::	is something in which you would modify the code so you could train to have several outputs on and use articulatory features::	0
"mn007"::	Mm-hmm.::	0
"me013"::	cuz then that would - that would go -::	0
"me013"::	that would be much  broader  and cover many different  situations.::	0
"me013"::	But if you go to  very  very  fine  categories, it's  very   binary.::	0
"mn007"::	Mmm.  Yeah, but I think -::	0
"mn007"::	Yeah, perhaps you're right, but you have more classes so  you - you have more information in your features. So,::	0
"me013"::	Mm-hmm.::	0
"mn007"::	Um  You have more information in the::	0
"me013"::	True.::	0
"mn007"::	uh posteriors vector::	0
"mn007"::	um::	0
"mn007"::	which means that -::	0
"mn007"::	But still the information is relevant because it's - it's information that helps to discriminate,::	0
"me013"::	Mm-hmm.::	0
"mn007"::	if it's possible to be able to discriminate::	0
"me013"::	Mm-hmm.::	0
"mn007"::	among the phonemes in context.::	0
"me013"::	Well it's - it's -::	0
"mn007"::	But the -::	0
"me013"::	it's an interesting thought. I mean we - we could disagree about it at length::	1
"mn007"::	Mmm.::	0
"mn007"::	Mmm.::	0
"me013"::	but the - the  real  thing is if you're interested in it you'll probably try it and -::	1
"me013"::	and  we'll see.::	0
"me013"::	But - but what I'm  more  concerned with now, as an operational level, is::	1
"mn007"::	Mmm.::	0
"me013"::	uh, you know, what do we do in four or five days?::	0
"me013"::	Uh, and -::	0
"me013"::	so we have  to be concerned  with::	0
"me013"::	Are we gonna look at any combinations of things, you know once the nets get retrained so you have this problem out of it.::	0
"mn007"::	Mmm.::	0
"me013"::	Um, are we going to look at  multi-band? Are we gonna look at combinations of things?::	0
"me013"::	Uh, what  questions  are we gonna ask,::	0
"me013"::	uh now that, I mean,::	0
"me013"::	we should probably turn shortly to this O_G_ I  note.::	0
"me013"::	Um, how are we going to  combine::	0
"me013"::	with what  they've  been focusing on?::	0
"me013"::	Uh,::	0
"me013"::	Uh we haven't been doing any of the L_D_ A  RASTA sort of thing.::	0
"mn007"::	Mm-hmm.::	0
"me013"::	And they, although they don't  talk  about it in this  note,  um,::	0
"me013"::	there's um,  the issue of the::	0
"me013"::	um  Mu law   business  uh  versus the logarithm,::	0
"me013"::	um,  so.::	0
"mn007"::	Mm-hmm.::	0
"me013"::	So what i- what is going on right  now?  What's right - you've got::	0
"me013"::	nets retraining,::	0
"me013"::	Are there - is there - are there any H_T_ K   trainings - testings going on?::	1
"mn007"::	N-::	0
"fn002"::	I - I - I'm trying the H_T_K with eh,::	1
"fn002"::	P_L_P twelve on-line delta-delta and  M_S_G   filter  together.::	1
"me013"::	The combination, I see.::	0
"fn002"::	The combination, yeah. But I haven't result  at this moment.::	1
"me013"::	M_S_G and - and P_L_P.::	0
"fn002"::	Yeah.::	0
"me013"::	And is this with the revised  on-line normalization?::	0
"fn002"::	Ye- Uh, with the old   older , yeah.::	0
"mn007"::	Yeah.::	0
"me013"::	Old  one.::	0
"me013"::	So it's using all the nets for that but again we have the hope that it -::	0
"fn002"::	Yeah.  But  We can::	0
"me013"::	We have the hope that it -::	0
"fn002"::	know soon.::	0
"me013"::	maybe it's not making too much difference,::	0
"fn002"::	Maybe.::	0
"me013"::	but - but yeah.::	0
"fn002"::	I don't know.::	0
"mn007"::	Yeah.::	0
"me013"::	Uh, O_K.::	0
"mn007"::	Uh so there is this combination, yeah. Working on combination obviously.::	0
"mn007"::	Um, I will start work on multi-band.::	0
"fn002"::	Mm-hmm.::	0
"mn007"::	And  we::	1
"mn007"::	plan to work also on the idea of using both::	0
"mn007"::	features  and net outputs.::	1
"fn002"::	Yep.::	0
"mn007"::	Um.::	0
"mn007"::	And  we think that::	0
"mn007"::	with this approach perhaps::	0
"mn007"::	we could reduce the number of outputs of the neural network.::	0
"mn007"::	Um,::	0
"mn007"::	So, get simpler networks,::	0
"mn007"::	because we still have the features.::	0
"mn007"::	So we have um::	1
"mn007"::	come up with um::	0
"mn007"::	different kind of  broad phonetic categories.::	1
"mn007"::	And we have -::	1
"mn007"::	Basically we have three  types of broad phonetic classes.::	0
"mn007"::	Well, something using place of articulation which - which leads to  nine, I think,  broad classes.::	1
"mn007"::	Uh, another which is based on manner, which is - is also something like nine classes.::	0
"mn007"::	And then,  something that combine both,::	0
"mn007"::	and we have  twenty f-  twenty-five?::	0
"me006"::	Twenty-seven.::	0
"mn007"::	Twenty-seven broad classes.::	1
"mn007"::	So like, uh,  oh, I don't know,::	0
"mn007"::	like back vowels, front vowels.::	0
"me013"::	So what you do -::	0
"mn007"::	Um::	0
"me013"::	um I just wanna understand so::	0
"me013"::	You have  two  net or  three  nets?  Was this?::	0
"me013"::	How many - how many nets do you have?::	0
"mn007"::	For the moments we do not - don't have nets, I mean,::	0
"me013"::	No nets.::	0
"mn007"::	It's just -::	0
"fn002"::	Begin to work in this.::	0
"mn007"::	Were we just changing  the labels to retrain nets  with fewer out- outputs.::	0
"fn002"::	We are   @@ .::	0
"me013"::	Right.::	0
"me013"::	But - but I didn't understand -::	0
"mn007"::	And then -::	0
"mn007"::	Mm-hmm.::	0
"me013"::	Uh.::	0
"me013"::	the software currently just has - uh a - allows for I think, the one - one hot::	0
"me013"::	output. So you're having multiple nets and combining them,  or - ?::	0
"me013"::	Uh, how are you - how are you coming up with -::	0
"me013"::	If you say  uh  If you have a  place   characteristic and a  manner  characteristic, how do you -::	0
"mn007"::	It-::	0
"me018"::	I think they have one output.::	0
"mn007"::	It's the single net, yeah.::	0
"me013"::	Oh, it's just one net.::	0
"me006"::	mm-hmm::	0
"fn002"::	Yeah.::	0
"mn007"::	It's one net with::	0
"mn007"::	um  twenty-seven outputs if we have twenty-seven classes, yeah.::	0
"me013"::	I see.::	0
"me013"::	I see, O_K.::	0
"mn007"::	So it's - Well, it's basically a standard net with fewer  classes.::	1
"me013"::	So you're sort of going the  other  way of what you were saying a bit ago instead of - yeah.::	0
"mn007"::	Yeah, but I think - Yeah.::	0
"me006"::	But  including  the  features.::	1
"mn007"::	B- b-::	0
"fn002"::	Yeah.::	0
"mn007"::	including  the features, yeah.::	0
"mn007"::	I don't think this  will work  alone.::	0
"mn007"::	I think it will get worse because::	1
"me013"::	Uh-huh.::	0
"mn007"::	Well, I believe the effect that - of - of too::	0
"mn007"::	reducing too much the information is::	0
"mn007"::	basically - basically what happens and -::	1
"mn007"::	but -::	0
"me013"::	But you think if you include that::	1
"me013"::	plus  the other features,::	1
"mn007"::	Yeah, because  there is perhaps one important thing that the net::	0
"mn007"::	brings, and O_G_I  show-   showed  that, is::	0
"mn007"::	the distinction between  sp- speech and  silence::	0
"mn007"::	Because these nets are trained on well-controlled condition. I mean the labels are obtained on clean speech, and we add noise after.::	0
"mn007"::	So this is one thing::	0
"mn007"::	And::	0
"mn007"::	But perhaps, something intermediary using also::	0
"mn007"::	some broad classes could - could bring so much more information.::	0
"mn007"::	Uh.::	0
"me013"::	So - so again then we have these broad classes::	0
"me013"::	and -::	0
"me013"::	well,  somewhat  broad. I mean, it's twenty-seven instead of sixty-four,::	0
"me013"::	basically.::	0
"mn007"::	Yeah.::	0
"me013"::	And you have the  original  features.::	0
"mn007"::	Yeah.::	0
"me013"::	Which are P_L_P, or something.::	0
"mn007"::	Mm-hmm.::	0
"me013"::	And then uh, just to remind me, all of that goes  into -::	1
"me013"::	uh, that all of that is transformed by uh, uh, K_- K_L or something, or - ?::	1
"mn007"::	There will probably be, yeah, one single K_L to transform everything or::	1
"fn002"::	Mu.::	0
"me013"::	Right.::	0
"mn007"::	uh, per-::	0
"fn002"::	No transform the P_L_P and only::	0
"fn002"::	transform the other::	0
"fn002"::	I'm not sure.::	0
"mn007"::	This is  still something  that yeah, we  don't know -::	0
"me013"::	Well no, I think -::	0
"me013"::	I  see.::	0
"fn002"::	Two e-::	0
"me013"::	So there's a question of whether you would -::	1
"mn007"::	Yeah.::	0
"me013"::	Right.::	0
"me013"::	Whether you would transform  together  or just  one.::	1
"fn002"::	@@  it's one.::	0
"me013"::	Yeah.::	0
"me013"::	Might wanna try it both ways.::	1
"me013"::	But that's  interesting.::	0
"me013"::	So that's something that you're - you  haven't  trained yet but are  preparing  to train, and -::	0
"mn007"::	Yeah.::	0
"me013"::	Yeah.::	0
"me013"::	Um::	0
"mn007"::	Mmm.::	0
"me013"::	Yeah, so I think Hynek will be here Monday.::	1
"me013"::	Monday or Tuesday. So::	0
"mn007"::	Uh, yeah.::	0
"me013"::	So I think, you know, we need to::	1
"me013"::	choose the - choose the experiments carefully,::	1
"me013"::	so we can get uh key -::	1
"me013"::	key questions answered::	0
"mn007"::	Mm-hmm.::	0
"me013"::	uh  before  then and::	1
"me013"::	leave other ones  aside  even if it::	0
"me013"::	leaves incomplete  tables   someplace, uh::	0
"me013"::	uh, it's - it's really time to -::	0
"me013"::	time to choose.::	0
"mn007"::	Mm-hmm.::	0
"me013"::	Um, let me pass this out,::	0
"me013"::	by the way.::	0
"me013"::	Um::	0
"me013"::	These are -::	0
"me013"::	Did - did -::	0
"me013"::	did I interrupt you? Were there other things that you wanted to -::	0
"fn002"::	Yeah, I have one.::	0
"mn007"::	Uh, no. I don't think so.::	0
"fn002"::	@@::	0
"mn007"::	Yeah, I have one.::	1
"me026"::	Oh, thanks.::	0
"me013"::	Ah!  O_K.::	0
"fn002"::	We have one.  @@::	0
"me013"::	O_K, we have  lots of them.::	0
"me013"::	O_K, so  um,::	0
"me013"::	Something I asked -::	0
"me013"::	So they're - they're doing  the - the V_A_D  I guess they mean::	1
"me013"::	voice  activity  detection::	0
"me013"::	So again, it's the silence -::	0
"me013"::	So they've just trained up a net::	1
"me013"::	which has two outputs, I believe.::	0
"me013"::	Um::	0
"me013"::	I asked uh  Hynek whether -::	0
"me013"::	I haven't talked to Sunil - I asked Hynek whether::	0
"me013"::	they compared that to::	0
"me013"::	just taking the nets we already  had   and summing up the probabilities.::	0
"mn007"::	Mm-hmm.::	0
"me013"::	Uh.::	0
"me013"::	To get the speech -::	0
"me013"::	voice activity detection, or else just using the silence,::	0
"me013"::	if there's only  one   silence output.::	0
"me013"::	Um  And, he didn't think they  had,  um.::	0
"me013"::	But on the other hand, maybe they can get by with a smaller net::	0
"me013"::	and  maybe  sometimes you don't  run  the other, maybe there's a  computational  advantage to having a separate net, anyway.::	0
"mn007"::	Mm-hmm.::	0
"me013"::	So um::	1
"me013"::	Their uh -::	0
"me013"::	the  results  look pretty  good.::	1
"mn007"::	Yeah.::	0
"me013"::	Um,  I mean, not  uniformly.::	0
"me013"::	I mean, there's a - an example or two  that you can find, where it made it slightly worse, but::	0
"me013"::	uh in - in  all  but a  couple::	0
"mn007"::	Mmm.::	0
"me013"::	examples.::	0
"me013"::	Uh.::	0
"fn002"::	But they have a question of the result. Um how are  trained  the - the L_D_A filter?::	0
"fn002"::	How  obtained  the L_D_A filter?::	0
"mn007"::	Mmm.::	1
"me013"::	I- I'm sorry. I don't understand your question.::	0
"fn002"::	Yes, um the L_D_A filter::	0
"fn002"::	needs some  training set::	0
"fn002"::	to obtain the filter.::	0
"fn002"::	Maybe::	0
"fn002"::	I don't know exactly  how   they are obtained.::	0
"me013"::	It's on   training.::	0
"fn002"::	Training, with the training test of each -::	0
"fn002"::	You understand me?::	0
"me013"::	No.::	0
"fn002"::	Yeah, uh for example,::	0
"fn002"::	L_D_A filter::	0
"fn002"::	need a set of -::	0
"fn002"::	a set of training  to obtain the filter.::	0
"fn002"::	And maybe  for the Italian, for the  T_D    T_E  on for Finnish, these filter are - are obtained with their own training set.::	0
"me013"::	Yes.::	0
"me013"::	Yes, I don't know.::	0
"me013"::	That's - that's - so that's a -::	0
"me013"::	that's a very good question, then -::	0
"me013"::	now that it -::	0
"me013"::	I understand it. It's::	0
"me013"::	"yeah, where does the L_D_A come from?" In the -::	0
"me013"::	In   earlier  experiments, they had taken L_D_A::	0
"me013"::	from a  completely  different  database,  right?::	0
"fn002"::	Yeah.::	0
"fn002"::	Yeah, because maybe it the same situation that the neural network training with their own::	0
"mn007"::	Mmm.::	0
"fn002"::	set.::	0
"me013"::	So that's a good question.   Where  does it come from?::	0
"me013"::	Yeah, I don't know.::	0
"me013"::	Um,::	0
"me013"::	but uh to tell you the  truth, I wasn't actually looking at the L_D_A so much when I - I was looking at it I was  mostly thinking about the -  the V_A_D.::	0
"me013"::	And um, it ap-::	0
"me013"::	it ap-::	0
"me013"::	Oh what does - what does A_S_P?::	0
"me013"::	Oh that's -::	0
"mn007"::	The features, yeah.::	0
"mn007"::	Yeah.::	0
"fn002"::	I don't understand also what is -::	0
"me013"::	It says "baseline A_S_P".::	0
"fn002"::	what is the  difference  between A_S_P and uh baseline  over ?::	0
"mn007"::	Yeah, I don't know.::	0
"me034"::	A_S_P.::	0
"fn002"::	This is -::	0
"me034"::	Oh.::	0
"me013"::	Anybody know   any  -::	0
"me034"::	There  it is.::	0
"me013"::	Um::	0
"me013"::	Cuz there's "baseline  Aurora "   above  it.::	0
"me034"::	Mm-hmm.::	0
"me013"::	And it's -::	0
"me013"::	This is  mostly  better than baseline, although in some cases it's a little worse, in a couple cases.::	0
"me034"::	Well, it says baseline A_S_P is twenty-three mill::	0
"fn002"::	Yeah.::	0
"me034"::	minus thirteen.::	0
"me013"::	Yeah, it  says  what it  is.  But I don't how that's different  from -::	0
"me034"::	From the baseline.  O_K.::	0
"me013"::	I think this was -::	0
"fn002"::	Yeah.::	0
"me013"::	I think this is the same point we were at::	0
"mn007"::	I think -::	0
"me013"::	when - when we were up in Oregon.::	0
"mn007"::	I think it's the C_zero - using C_zero instead of log energy. Yeah, it's  this.::	0
"fn002"::	Ah, O_K, mm-hmm.::	0
"fn002"::	yeah.::	0
"me013"::	Oh.::	0
"me013"::	O_K.::	0
"mn007"::	It should be that, yeah.::	0
"mn007"::	Because -::	0
"me013"::	Shouldn't it be -::	0
"me018"::	They s-::	0
"me018"::	they say in here that the V_A_D is  not  used as an additional feature. Does - does anybody know  how  they're using it?::	0
"me013"::	Yeah.  So - so what they're doing here is,::	0
"mn007"::	Yeah.::	0
"me013"::	i- if you look down at the block diagram,::	0
"me013"::	um,::	0
"me013"::	they estimate - they get a -::	0
"me018"::	But that -::	0
"me013"::	they get an estimate  of whether it's speech or silence,::	0
"me013"::	and then they have a median  filter  of it.::	0
"me018"::	Mm-hmm.::	0
"me013"::	And so um,::	0
"me013"::	basically they're trying to find  stretches.::	0
"me013"::	The median filter is enforcing a - i- it having some  continuity.::	0
"me018"::	Mm-hmm.::	0
"me013"::	You find  stretches  where the  combination of the  frame wise V_A_D and the -::	0
"me013"::	the median  filter  say that there's a stretch of  silence.::	0
"me013"::	And then it's going through and just throwing the  data  away.::	0
"me034"::	Hmm.::	0
"me013"::	Right?::	0
"me013"::	So um -::	0
"me018"::	So it's - it's -::	0
"me018"::	I don't understand. You mean it's throwing out frames?::	0
"me013"::	It's throwing out chunks of frames, yeah.::	0
"me018"::	Before -::	0
"me013"::	There's - the - the median filter is enforcing that it's  not  gonna be single cases of frames, or isolated frames.::	0
"me018"::	Yeah.::	0
"me013"::	So it's throwing out frames::	0
"me013"::	and the thing is  um,::	0
"me013"::	what I don't understand is how they're doing this with H_T_ K.::	0
"me013"::	This is -::	0
"me018"::	Yeah, that's what I was just gonna ask. How can you just throw out frames?::	0
"me013"::	Yeah.::	0
"me013"::	Well,::	0
"mn007"::	i-::	0
"me013"::	you - you  can,  right? I mean y- you - you -::	0
"me013"::	it  stretches  again. For single frames I think it would be pretty  hard.  But if you say speech starts here, speech ends there.  Right?::	0
"mn007"::	Yeah.::	0
"me018"::	Yeah.::	0
"me018"::	Mm-hmm.::	0
"me034"::	Huh.::	0
"mn007"::	Yeah.::	0
"mn007"::	Yeah, you can basically remove the - the frames from the feature - feature files. And.::	0
"me013"::	Yeah.::	0
"me013"::	Yeah, so I mean in the - i- i- in the - in the  decoding,  you're saying that we're gonna decode from here to here.::	0
"mn007"::	I t-::	0
"me018"::	Mm-hmm.::	0
"me013"::	I think they're - they're - they're treating it,::	0
"me013"::	you know, like uh -::	0
"me013"::	well, it's not isolated word, but - but connected, you know, the - the -::	0
"me018"::	In the text they say that this - this is a  tentative  block diagram of a  possible  configuration we could  think  of.::	0
"me018"::	So that sort of sounds like they're not  doing  that yet.::	0
"me013"::	Well.  No they - they have  numbers  though, right?  So I think they're - they're doing something like that. I think that they're - they're -::	0
"me013"::	I think what I mean by tha- that is they're trying to come up with a block diagram that's plausible for the standard.::	0
"me013"::	In other words, it's - uh -::	0
"me013"::	I mean from the point of view of - of uh reducing the number of bits you have to transmit it's not a bad idea to detect silence  anyway.::	0
"me018"::	Yeah.::	0
"me018"::	Yeah.::	0
"me018"::	I'm just wondering what exactly  did  they do up in  this  table if it wasn't this.::	0
"me013"::	Um.::	0
"me013"::	But it's - the thing is it's that - that - that's - that's I - I -::	0
"me013"::	Certainly it would be tricky about it intrans- in transmitting  voice,::	0
"me013"::	uh uh for  listening  to, is that these kinds of things::	0
"me013"::	uh cut  speech  off  a lot. Right?::	0
"me013"::	And so  um::	0
"me018"::	Mm-hmm.::	0
"me018"::	Plus it's gonna introduce delays.::	0
"me013"::	It  does  introduce delays but they're claiming that it's - it's within the -::	0
"me013"::	the boundaries of it.::	0
"me018"::	Mmm.::	0
"me013"::	And the L_D_A introduces delays, and b-  what he's suggesting this here is a parallel path so that it doesn't introduce::	0
"me013"::	uh, any more delay.::	0
"me013"::	I- it introduces two hundred milliseconds of delay but at the same  time the L_D_A  down here -::	0
"me013"::	I  don't know -  Wh- what's the difference between  T_L_D_A  and  S_L_D_A ?::	0
"me034"::	Temporal and spectral.::	0
"me013"::	Ah, thank you.::	0
"fn002"::	Temporal  L_D_A.::	0
"me013"::	Yeah, you  would  know that.::	0
"me034"::	Yeah::	0
"me013"::	So um.::	0
"me013"::	The  temporal  L_D_A does in fact include the same -::	0
"me013"::	so that - I think::	0
"me013"::	he - well, by -::	0
"me013"::	by saying this is a b- a tentative block di- diagram I think means::	0
"me013"::	if you construct it this way, this - this delay would work in that way and then it'd be O_K.::	0
"me018"::	Ah.::	0
"me013"::	They - they clearly  did  actually  remove    silent  sections in order - because they::	0
"me013"::	got these   word  error rate  results.::	0
"me013"::	So um::	1
"me013"::	I think that it's - it's  nice  to do that in this because in fact, it's gonna give a better word error result::	1
"me013"::	and therefore will help within an evaluation.::	1
"me013"::	Whereas to whether this would actually be in a final standard, I don't know.::	0
"me013"::	Um.::	1
"me013"::	Uh, as you know,  part  of the problem with evaluation right now is that the  word models are pretty bad and nobody wants -::	1
"me013"::	has - has approached improving them.::	1
"me013"::	So  it's possible that a lot of the problems::	0
"me013"::	with so many insertions and so forth would go away if they were better word models  to begin with.::	0
"me013"::	So::	0
"me013"::	this might just be a temporary thing. But -::	0
"me013"::	But, on the  other  hand, and maybe - maybe it's a decent idea.::	0
"me013"::	So um::	1
"me013"::	The question we're gonna wanna go  through next week when Hynek shows up I guess is given that we've been -::	0
"me013"::	if you look at what we've been trying,::	1
"me013"::	we're uh looking at::	1
"me013"::	uh, by  then  I guess,::	1
"me013"::	combinations of features and multi-band::	1
"me013"::	Uh, and we've been looking at  cross-language, cross  task  issues.::	1
"me013"::	And they've been not so much looking at::	0
"me013"::	the cross task uh multiple language issues.::	0
"me013"::	But they've been looking at uh -  at these issues. At the::	1
"me013"::	on-line normalization and the uh::	0
"me013"::	voice activity detection.::	1
"me013"::	And I guess when he comes here we're gonna have to start deciding about::	1
"me013"::	um what do we choose::	0
"me013"::	from what we've looked at::	0
"me013"::	to um blend with  some group of things in what they've looked at::	0
"me013"::	And once we  choose  that,::	1
"me013"::	how do we split up the  effort?::	1
"me013"::	Uh, because we still have - even once we  choose,::	0
"me013"::	we've still got  uh another::	0
"me013"::	month or so, I mean there's holidays in the way,::	0
"me013"::	but - but uh::	0
"me013"::	I  think  the evaluation data comes January thirty- first  so there's still a fair amount of time::	0
"me013"::	to  do  things together it's just that they probably should be somewhat more  coherent  between the two  sites::	0
"me013"::	in that - that amount of time.::	0
"me018"::	When they removed the silence frames, did they insert some kind of a marker so that the recognizer knows it's -  knows when it's time to back trace or something?::	0
"me013"::	Well, see they, I - I think they're::	0
"me013"::	Um.::	0
"me013"::	I don't know the -::	0
"me013"::	the specifics of how they're doing it. They're -::	0
"me013"::	they're getting around the way the recognizer works because they're not allowed to::	0
"me013"::	um,  change  the  scripts::	0
"me018"::	Oh, right.::	0
"me013"::	for the  recognizer,::	0
"me013"::	I believe. So. Uh.::	0
"me018"::	Maybe they're just inserting some nummy frames or something?::	0
"me013"::	Uh, you know  that's  what  I  had thought.::	0
"me013"::	But I don't - I don't think they  are.::	0
"me013"::	I mean that's - sort of what - the way  I  had imagined would  happen  is that on the other side, yeah you::	0
"me018"::	Hmm.::	0
"me013"::	p- put some low level noise or something.::	0
"me013"::	Probably don't want all zeros. Most recognizers don't like zeros but::	0
"me018"::	Hmm.::	0
"me013"::	but  you know,::	0
"me018"::	Yeah.::	0
"me013"::	put some epsilon in or some rand- sorry epsilon random variable::	0
"me013"::	in or something.::	0
"me018"::	Some constant vector.::	0
"me013"::	Maybe not a constant but it doesn't, uh - don't like to divide by the variance of that,::	0
"me018"::	I mean i- w-::	0
"me018"::	Or something -::	0
"me013"::	but I mean it's::	0
"me018"::	That's right. But something that - what I mean is something that is  very distinguishable from  speech.::	0
"me013"::	Mm-hmm.::	0
"me018"::	So that the - the  silence  model in H_T_K will always pick it up.::	0
"me013"::	Yeah. So I - I - that's what I thought they would do.::	0
"me013"::	or else, uh  uh maybe there  is  some indicator to tell it to start and stop, I don't know.::	0
"me018"::	Hmm.::	0
"me013"::	But  whatever  they did, I mean they have to play within the rules of this specific evaluation.::	0
"me018"::	Yeah.::	0
"me013"::	We c- we can find out.::	0
"me018"::	Cuz you gotta do  something.   Otherwise,  if it's just a bunch of  speech,   stuck   together  -::	0
"me013"::	No  they're -::	0
"me018"::	Yeah.::	0
"me013"::	It would do  badly  and it didn't so  badly,  right? So  they  did  something.::	0
"me018"::	Yeah, right.::	0
"me018"::	Yeah, yeah.::	0
"me013"::	Yeah.::	0
"me013"::	Uh.::	0
"me013"::	So, O_K, So I think::	0
"me013"::	this brings  me  up to date a bit.::	0
"me013"::	It hopefully brings other  people up to date a bit.::	0
"me013"::	And um::	0
"me013"::	Um  I think -::	0
"me013"::	Uh, I wanna look at these numbers off-line a little bit and think about it and -  and talk with everybody uh,  outside of this meeting.::	0
"me013"::	Um, but uh::	0
"me013"::	No I mean it sounds like - I mean::	0
"me013"::	there - there - there are the usual number of - of::	0
"me013"::	little - little problems and bugs and so forth but it sounds like they're getting ironed out.::	0
"me013"::	And now we're::	0
"me013"::	seem to be kind of in a position to actually  uh,::	0
"me013"::	look  at stuff and - and - and  compare  things.::	0
"me013"::	So I think that's - that's pretty good.::	0
"me013"::	Um::	0
"me013"::	I don't know what the -::	0
"me013"::	One of the things I wonder about,::	0
"me013"::	coming back to the first results you talked about, is - is::	0
"me013"::	how much,  uh  things could be helped::	0
"me013"::	by more parameters.::	0
"me013"::	And uh -::	0
"me013"::	And uh how many more parameters we can afford to  have,::	0
"me013"::	in terms of the uh computational limits.::	0
"me013"::	Because  anyway  when we go to::	0
"me013"::	twice  as much  data::	0
"me013"::	and have the  same  number of  parameters,::	0
"me013"::	particularly when it's twice as much data  and  it's quite  diverse,::	0
"me013"::	um, I wonder if having twice as many parameters would  help.::	0
"me013"::	Uh, just have a bigger hidden  layer.::	0
"mn007"::	Mm-hmm.::	0
"me013"::	Uh::	0
"me013"::	But -::	0
"me013"::	I  doubt  it would  help by forty per  cent.::	0
"me013"::	But::	0
"mn007"::	Yeah.::	0
"me013"::	but uh::	0
"me013"::	Just  curious.::	0
"me013"::	How are we doing on the::	0
"me013"::	resources? Disk, and -::	0
"mn007"::	I think we're alright, um,  not much problems with that.::	0
"me013"::	O_K.::	0
"me013"::	Computation?::	0
"mn007"::	It's O_K.::	0
"mn007"::	Well this table took uh  more than five days to  get   back .::	1
"me013"::	We -::	0
"me013"::	Yeah. Yeah, well.::	0
"mn007"::	But -  Yeah.::	0
"me013"::	Are - were you folks using Gin?  That's a - that just died, you know?::	0
"mn007"::	Mmm, no. You were using Gin  perhaps, yeah?  No.::	1
"fn002"::	No.::	0
"me006"::	It just died.::	0
"me013"::	No? Oh, that's good. O_K.::	0
"me013"::	Yeah,::	0
"me013"::	we're gonna get a replacement::	0
"fn002"::	Yes.::	0
"me013"::	server that'll be a faster server,::	0
"me013"::	actually. That'll be - It's a::	0
"mn007"::	Hmm.::	1
"me013"::	seven hundred fifty megahertz uh SUN::	0
"me034"::	Tonic.::	0
"me013"::	uh::	0
"me013"::	But it won't be installed for  a little while.::	0
"mn007"::	Mm-hmm.::	1
"me013"::	U-::	0
"me026"::	Do we -::	0
"me013"::	Go ahead.::	0
"me026"::	Do we have that big new I_B_M machine the, I think in th-::	0
"me013"::	We have the  little tiny I_B_M machine::	1
"me013"::	that might someday grow up to be a big  I_B_M machine.::	1
"me013"::	It's got s- slots for eight,::	0
"me013"::	uh I_B_M was donating  five,  I think we only got  two  so far,::	1
"me013"::	processors.::	0
"me013"::	We had originally hoped we were getting eight hundred megahertz processors. They ended up being five fifty.::	0
"me013"::	So instead of having eight processors that were eight hundred megahertz, we ended up with two  that are five hundred and fifty megahertz.::	0
"me013"::	And more are supposed to come soon and there's only a moderate amount of dat- of memory. So I don't think::	0
"me013"::	anybody has been sufficiently excited by it to::	0
"me013"::	spend much time::	0
"me013"::	uh  with it, but uh::	0
"me013"::	Hopefully,  they'll get us some more::	0
"me013"::	parts, soon and -::	0
"me013"::	Uh, yeah, I think that'll be - once we get it populated,::	0
"me013"::	that'll be a nice  machine.  I mean we  will  ultimately get eight processors in there.::	0
"me013"::	And uh - and uh a nice amount of memory. Uh so it'll be a pr- pretty fast Linux machine.::	0
"me026"::	And if we can do things on Linux,  some of the machines we have going already, like Swede?::	0
"me013"::	Mm-hmm.::	0
"me026"::	Um::	0
"me026"::	It seems pretty fast.::	0
"me013"::	Mm-hmm.::	0
"me026"::	But -::	0
"me026"::	I think Fudge is pretty fast too.::	0
"me013"::	Yeah, I mean you can  check  with uh  Dave  Johnson.  I mean, it - it's -::	1
"me013"::	I  think  the machine is just  sitting  there.::	0
"me013"::	And it  does  have two  processors,  you know and -::	1
"me013"::	Somebody could do -::	0
"me013"::	you know, uh, check out::	0
"me013"::	uh the multi-threading  libraries. And::	1
"me013"::	I mean i- it's possible that the -::	0
"me013"::	I mean, I guess the  prudent  thing to do would be for somebody to do the work on -::	1
"me013"::	on getting our code running::	0
"me013"::	on that machine with  two  processors  even though there  aren't  five or eight.::	1
"me013"::	There's - there's - there's gonna be  debugging  hassles::	1
"me013"::	and then we'd be set for when we  did  have five or eight, to have it really be useful.::	1
"me013"::	But.::	1
"me013"::	Notice how I said somebody and::	1
"me013"::	turned my head your direction. That's one thing you don't get in these recordings. You don't get the -::	1
"me013"::	don't get the visuals but -::	0
"me026"::	I- is it um  mostly um the neural network trainings that are  um slowing us down or the H_T_K runs that are slowing us down?::	0
"me013"::	Uh, I think yes.::	0
"me013"::	Uh,::	0
"me013"::	Isn't that right? I mean I think you're - you're sort of held up by both, right?::	0
"me013"::	If the - if the neural net trainings were a hundred times faster::	0
"me013"::	you still wouldn't  be anything -::	0
"me013"::	running through these a hundred times faster because you'd::	0
"me013"::	be stuck by the H_T_K trainings, right?::	0
"mn007"::	Mmm.::	0
"mn007"::	Yeah.::	0
"me013"::	But if the H_T_K - I mean I think they're both -::	0
"me013"::	It sounded like they were roughly equal?::	0
"me013"::	Is that about right?::	0
"mn007"::	Yeah.::	0
"me013"::	Yeah.::	1
"me026"::	Because, um  I  think  that'll be running Linux, and Sw- Swede and Fudge are already running Linux so,  um I could try to::	1
"me026"::	get  um the train- the neural network trainings or the H_T_K stuff running under Linux, and to start with I'm  wondering which one I should pick first.::	1
"me013"::	Uh, probably the neural net cuz it's probably - it - it's -::	1
"me013"::	it's um -::	0
"me013"::	Well, I - I don't know.::	0
"me013"::	They both -::	0
"me013"::	H_T_K we use for::	0
"me013"::	um::	0
"me013"::	this Aurora stuff::	0
"me013"::	Um::	0
"me013"::	Um, I think::	0
"me013"::	It's not  clear  yet what we're gonna use::	0
"me013"::	for trainings uh -::	0
"me013"::	Well,::	0
"me013"::	there's the trainings uh - is it the  training  that takes the time, or the  decoding?::	0
"me013"::	Uh, is it about equal  between the two?::	1
"me013"::	For - for Aurora?::	0
"mn007"::	For H_T_K?::	0
"me013"::	For - Yeah. For the Aurora?::	0
"mn007"::	Uh::	0
"mn007"::	Training  is longer.::	0
"me013"::	O_K.::	0
"mn007"::	Yeah.::	0
"me013"::	O_K.::	0
"me013"::	Well, I don't know how we can -::	0
"me013"::	I don't know how to -::	1
"me013"::	Do we have H_T_K source? Is that -::	0
"mn007"::	Mmm.::	1
"me013"::	Yeah.::	0
"me013"::	You would  think  that would fairly trivially -::	0
"me013"::	the  training  would,  anyway,  th- the  testing::	0
"me013"::	uh I don't - I don't::	0
"me013"::	think would::	0
"me013"::	parallelize all that well.::	0
"me013"::	But I think  that  you could::	0
"me013"::	certainly do d- um,::	0
"me013"::	distributed, sort of -  Ah, no, it's the -::	0
"me013"::	each individual::	0
"me013"::	sentence::	0
"me013"::	is pretty tricky to parallelize.::	0
"me013"::	But you could split up the sentences in a test set.::	0
"me018"::	They  have  a - they have a thing for  doing  that and th- they have for  awhile,  in H_T_ K.::	0
"me013"::	Yeah?::	0
"me018"::	And you can parallelize the training.::	0
"me018"::	And run it on several machines and it just basically keeps counts.::	0
"me013"::	Aha!::	0
"me018"::	And there's something -::	0
"me018"::	a final  thing that you run and it accumulates all the counts together.::	0
"me013"::	I see.::	0
"mn007"::	Mmm.::	0
"me018"::	I don't what their scripts are  set up to do for the  Aurora  stuff, but -::	0
"mn007"::	Yeah.::	0
"me013"::	Something that we haven't really settled on yet is other than::	0
"me013"::	this  Aurora  stuff,  uh what do we do, large vocabulary::	0
"me013"::	training slash testing::	0
"me013"::	for uh tandem systems.::	0
"me013"::	Cuz we hadn't really  done  much with tandem systems for larger stuff.::	0
"me013"::	Cuz we had this one collaboration with C_M_U and we used SPHINX.::	0
"me013"::	Uh, we're also gonna be collaborating with S_R_I and we have their - have  theirs.::	0
"me013"::	Um  So::	0
"me013"::	I don't know::	0
"me013"::	Um.::	0
"me013"::	So I - I think the - the advantage of going with the neural net thing is that we're gonna use the neural net trainings,::	0
"me013"::	no matter what,::	0
"me026"::	O_K.::	0
"me013"::	for a lot of the things we're doing,::	0
"me013"::	whereas, w- exactly which::	0
"me013"::	H_M_M - Gaussian-mixture-based H_M_M thing we use is gonna depend::	0
"me013"::	uh::	0
"me013"::	So with that,::	0
"me013"::	maybe we should uh::	0
"me013"::	go to our::	0
"me013"::	digit recitation task.::	0
"me013"::	And, it's about eleven fifty.::	0
"me013"::	Canned.::	0
"me013"::	Uh, I can - I can start over here.::	0
"me013"::	Great, uh, could you give Adam a call. Tell him to::	0
"me006"::	Oh.::	0
"me013"::	He's at two nine seven seven.::	0
"me013"::	O_K.  I think we can::	0
"me013"::	@@  You know Herve's coming tomorrow, right?::	0
"me013"::	Herve will be giving a talk, yeah, talk at eleven.::	0
"me006"::	Hello, is Adam there?::	0
"me006"::	Hey Adam, this is Barry.::	0
"me006"::	Yeah we're all done.::	0
"me006"::	O_K, thanks. Bye bye.::	0
"me013"::	Did uh, did everybody sign these consent::	0
"me013"::	Er everybody::	0
"me013"::	Has everyone signed a consent form before, on previous meetings? You don't have to do it again each time::	0
"me013"::	Yes.::	0
"me013"::	microphones off::	0
